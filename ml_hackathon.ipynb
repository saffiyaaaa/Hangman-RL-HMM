{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuyzZH7q14lG",
        "outputId": "eb7eff1a-0873-453b-99bc-bd931bdf935a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded 50000 words from corpus.txt\n",
            "   Length range: 1-24, avg: 9.5\n",
            "\n",
            "üìà Total sequences: 50000\n",
            "   Training on: 5000 words\n",
            "‚úÖ Smart initialization using corpus statistics\n",
            "   Top initial letters: [('A', 385), ('B', 223), ('C', 453), ('D', 252), ('E', 167)]\n",
            "\n",
            "==================================================\n",
            "üöÄ TRAINING HMM: 5000 sequences, 20 max iterations\n",
            "==================================================\n",
            "\n",
            "üìä Iteration 1/20\n",
            "   Processed 1000/5000 sequences...\n",
            "   Processed 2000/5000 sequences...\n",
            "   Processed 3000/5000 sequences...\n",
            "   Processed 4000/5000 sequences...\n",
            "   Processed 5000/5000 sequences...\n",
            "   Total LL: -123633.86\n",
            "   Avg LL per word: -24.7268\n",
            "   ‚≠ê New best model!\n",
            "\n",
            "üìä Iteration 2/20\n",
            "   Processed 1000/5000 sequences...\n",
            "   Processed 2000/5000 sequences...\n",
            "   Processed 3000/5000 sequences...\n",
            "   Processed 4000/5000 sequences...\n",
            "   Processed 5000/5000 sequences...\n",
            "   Total LL: -119885.19\n",
            "   Avg LL per word: -23.9770\n",
            "   Improvement: 3748.66\n",
            "   ‚≠ê New best model!\n",
            "\n",
            "üìä Iteration 3/20\n",
            "   Processed 1000/5000 sequences...\n",
            "   Processed 2000/5000 sequences...\n",
            "   Processed 3000/5000 sequences...\n",
            "   Processed 4000/5000 sequences...\n",
            "   Processed 5000/5000 sequences...\n",
            "   Total LL: -119273.01\n",
            "   Avg LL per word: -23.8546\n",
            "   Improvement: 612.18\n",
            "   ‚≠ê New best model!\n",
            "\n",
            "üìä Iteration 4/20\n",
            "   Processed 1000/5000 sequences...\n",
            "   Processed 2000/5000 sequences...\n",
            "   Processed 3000/5000 sequences...\n",
            "   Processed 4000/5000 sequences...\n",
            "   Processed 5000/5000 sequences...\n",
            "   Total LL: -119054.79\n",
            "   Avg LL per word: -23.8110\n",
            "   Improvement: 218.22\n",
            "   ‚≠ê New best model!\n",
            "\n",
            "üìä Iteration 5/20\n",
            "   Processed 1000/5000 sequences...\n",
            "   Processed 2000/5000 sequences...\n",
            "   Processed 3000/5000 sequences...\n",
            "   Processed 4000/5000 sequences...\n",
            "   Processed 5000/5000 sequences...\n",
            "   Total LL: -118938.92\n",
            "   Avg LL per word: -23.7878\n",
            "   Improvement: 115.87\n",
            "   ‚≠ê New best model!\n",
            "\n",
            "üìä Iteration 6/20\n",
            "   Processed 1000/5000 sequences...\n",
            "   Processed 2000/5000 sequences...\n",
            "   Processed 3000/5000 sequences...\n",
            "   Processed 4000/5000 sequences...\n",
            "   Processed 5000/5000 sequences...\n",
            "   Total LL: -118856.44\n",
            "   Avg LL per word: -23.7713\n",
            "   Improvement: 82.48\n",
            "   ‚≠ê New best model!\n",
            "\n",
            "üìä Iteration 7/20\n",
            "   Processed 1000/5000 sequences...\n",
            "   Processed 2000/5000 sequences...\n",
            "   Processed 3000/5000 sequences...\n",
            "   Processed 4000/5000 sequences...\n",
            "   Processed 5000/5000 sequences...\n",
            "   Total LL: -118783.78\n",
            "   Avg LL per word: -23.7568\n",
            "   Improvement: 72.67\n",
            "   ‚≠ê New best model!\n",
            "\n",
            "üìä Iteration 8/20\n",
            "   Processed 1000/5000 sequences...\n",
            "   Processed 2000/5000 sequences...\n",
            "   Processed 3000/5000 sequences...\n",
            "   Processed 4000/5000 sequences...\n",
            "   Processed 5000/5000 sequences...\n",
            "   Total LL: -118711.00\n",
            "   Avg LL per word: -23.7422\n",
            "   Improvement: 72.77\n",
            "   ‚≠ê New best model!\n",
            "\n",
            "üìä Iteration 9/20\n",
            "   Processed 1000/5000 sequences...\n",
            "   Processed 2000/5000 sequences...\n",
            "   Processed 3000/5000 sequences...\n",
            "   Processed 4000/5000 sequences...\n",
            "   Processed 5000/5000 sequences...\n",
            "   Total LL: -118635.58\n",
            "   Avg LL per word: -23.7271\n",
            "   Improvement: 75.43\n",
            "   ‚≠ê New best model!\n",
            "\n",
            "üìä Iteration 10/20\n",
            "   Processed 1000/5000 sequences...\n",
            "   Processed 2000/5000 sequences...\n",
            "   Processed 3000/5000 sequences...\n",
            "   Processed 4000/5000 sequences...\n",
            "   Processed 5000/5000 sequences...\n",
            "   Total LL: -118560.08\n",
            "   Avg LL per word: -23.7120\n",
            "   Improvement: 75.49\n",
            "   ‚≠ê New best model!\n",
            "\n",
            "üìä Iteration 11/20\n",
            "   Processed 1000/5000 sequences...\n",
            "   Processed 2000/5000 sequences...\n",
            "   Processed 3000/5000 sequences...\n",
            "   Processed 4000/5000 sequences...\n",
            "   Processed 5000/5000 sequences...\n",
            "   Total LL: -118486.08\n",
            "   Avg LL per word: -23.6972\n",
            "   Improvement: 74.00\n",
            "   ‚≠ê New best model!\n",
            "\n",
            "üìä Iteration 12/20\n",
            "   Processed 1000/5000 sequences...\n",
            "   Processed 2000/5000 sequences...\n",
            "   Processed 3000/5000 sequences...\n",
            "   Processed 4000/5000 sequences...\n",
            "   Processed 5000/5000 sequences...\n",
            "   Total LL: -118413.20\n",
            "   Avg LL per word: -23.6826\n",
            "   Improvement: 72.88\n",
            "   ‚≠ê New best model!\n",
            "\n",
            "üìä Iteration 13/20\n",
            "   Processed 1000/5000 sequences...\n",
            "   Processed 2000/5000 sequences...\n",
            "   Processed 3000/5000 sequences...\n",
            "   Processed 4000/5000 sequences...\n",
            "   Processed 5000/5000 sequences...\n",
            "   Total LL: -118343.02\n",
            "   Avg LL per word: -23.6686\n",
            "   Improvement: 70.18\n",
            "   ‚≠ê New best model!\n",
            "\n",
            "üìä Iteration 14/20\n",
            "   Processed 1000/5000 sequences...\n",
            "   Processed 2000/5000 sequences...\n",
            "   Processed 3000/5000 sequences...\n",
            "   Processed 4000/5000 sequences...\n",
            "   Processed 5000/5000 sequences...\n",
            "   Total LL: -118278.04\n",
            "   Avg LL per word: -23.6556\n",
            "   Improvement: 64.98\n",
            "   ‚≠ê New best model!\n",
            "\n",
            "üìä Iteration 15/20\n",
            "   Processed 1000/5000 sequences...\n",
            "   Processed 2000/5000 sequences...\n",
            "   Processed 3000/5000 sequences...\n",
            "   Processed 4000/5000 sequences...\n",
            "   Processed 5000/5000 sequences...\n",
            "   Total LL: -118219.45\n",
            "   Avg LL per word: -23.6439\n",
            "   Improvement: 58.59\n",
            "   ‚≠ê New best model!\n",
            "\n",
            "üìä Iteration 16/20\n",
            "   Processed 1000/5000 sequences...\n",
            "   Processed 2000/5000 sequences...\n",
            "   Processed 3000/5000 sequences...\n",
            "   Processed 4000/5000 sequences...\n",
            "   Processed 5000/5000 sequences...\n",
            "   Total LL: -118167.04\n",
            "   Avg LL per word: -23.6334\n",
            "   Improvement: 52.41\n",
            "   ‚≠ê New best model!\n",
            "\n",
            "üìä Iteration 17/20\n",
            "   Processed 1000/5000 sequences...\n",
            "   Processed 2000/5000 sequences...\n",
            "   Processed 3000/5000 sequences...\n",
            "   Processed 4000/5000 sequences...\n",
            "   Processed 5000/5000 sequences...\n",
            "   Total LL: -118119.64\n",
            "   Avg LL per word: -23.6239\n",
            "   Improvement: 47.41\n",
            "   ‚≠ê New best model!\n",
            "\n",
            "üìä Iteration 18/20\n",
            "   Processed 1000/5000 sequences...\n",
            "   Processed 2000/5000 sequences...\n",
            "   Processed 3000/5000 sequences...\n",
            "   Processed 4000/5000 sequences...\n",
            "   Processed 5000/5000 sequences...\n",
            "   Total LL: -118075.61\n",
            "   Avg LL per word: -23.6151\n",
            "   Improvement: 44.03\n",
            "   ‚≠ê New best model!\n",
            "\n",
            "üìä Iteration 19/20\n",
            "   Processed 1000/5000 sequences...\n",
            "   Processed 2000/5000 sequences...\n",
            "   Processed 3000/5000 sequences...\n",
            "   Processed 4000/5000 sequences...\n",
            "   Processed 5000/5000 sequences...\n",
            "   Total LL: -118033.43\n",
            "   Avg LL per word: -23.6067\n",
            "   Improvement: 42.19\n",
            "   ‚≠ê New best model!\n",
            "\n",
            "üìä Iteration 20/20\n",
            "   Processed 1000/5000 sequences...\n",
            "   Processed 2000/5000 sequences...\n",
            "   Processed 3000/5000 sequences...\n",
            "   Processed 4000/5000 sequences...\n",
            "   Processed 5000/5000 sequences...\n",
            "   Total LL: -117991.99\n",
            "   Avg LL per word: -23.5984\n",
            "   Improvement: 41.44\n",
            "   ‚≠ê New best model!\n",
            "\n",
            "==================================================\n",
            "‚úÖ TRAINING COMPLETE\n",
            "   Final LL: -117991.99\n",
            "   Best LL: -117991.99\n",
            "==================================================\n",
            "\n",
            "üì¶ Model saved as hmm_trained_v1.pkl\n",
            "\n",
            "üîç MODEL INSIGHTS:\n",
            "   Most likely initial states: ['J', 'P', 'C', 'U', 'A']\n",
            "   Emission matrix sparsity: 85.1%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import string\n",
        "import pickle\n",
        "from collections import Counter\n",
        "\n",
        "# =============================\n",
        "# ENHANCED HMM TRAINING\n",
        "# =============================\n",
        "\n",
        "ALPHABET = list(string.ascii_uppercase)\n",
        "SYM2IDX = {c: i for i, c in enumerate(ALPHABET)}\n",
        "IDX2SYM = {i: c for c, i in SYM2IDX.items()}\n",
        "\n",
        "def normalize_rows(M, eps=1e-12):\n",
        "    \"\"\"Normalize matrix rows to sum to 1\"\"\"\n",
        "    M = M + eps\n",
        "    return M / M.sum(axis=1, keepdims=True)\n",
        "\n",
        "def normalize_vec(v, eps=1e-12):\n",
        "    \"\"\"Normalize vector to sum to 1\"\"\"\n",
        "    v = v + eps\n",
        "    return v / v.sum()\n",
        "\n",
        "\n",
        "# ========== DATA LOADING ==========\n",
        "def load_corpus(path):\n",
        "    \"\"\"Load and preprocess corpus with statistics\"\"\"\n",
        "    words = []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            w = line.strip().upper()\n",
        "            w = ''.join([c for c in w if c in SYM2IDX])\n",
        "            if len(w) > 0:\n",
        "                words.append(w)\n",
        "\n",
        "    # Compute statistics\n",
        "    lengths = [len(w) for w in words]\n",
        "    print(f\"‚úÖ Loaded {len(words)} words from {path}\")\n",
        "    print(f\"   Length range: {min(lengths)}-{max(lengths)}, avg: {np.mean(lengths):.1f}\")\n",
        "\n",
        "    return words\n",
        "\n",
        "def words_to_observation_sequences(words):\n",
        "    \"\"\"Convert words to observation sequences\"\"\"\n",
        "    return [[SYM2IDX[c] for c in w] for w in words]\n",
        "\n",
        "\n",
        "# ========== SMART INITIALIZATION ==========\n",
        "def init_params_smart(words, n_states=26, n_obs=26):\n",
        "    \"\"\"\n",
        "    Initialize HMM parameters using corpus statistics\n",
        "    Much better than random initialization!\n",
        "    \"\"\"\n",
        "    # Count letter frequencies for initial state prior\n",
        "    first_letters = Counter([w[0] for w in words])\n",
        "    pi = np.array([first_letters.get(ALPHABET[i], 0) for i in range(n_states)], dtype=float)\n",
        "    pi = normalize_vec(pi)\n",
        "\n",
        "    # Count bigram transitions for state transition matrix\n",
        "    A = np.ones((n_states, n_states))  # Smoothing\n",
        "    for word in words:\n",
        "        for i in range(len(word) - 1):\n",
        "            curr_idx = SYM2IDX[word[i]]\n",
        "            next_idx = SYM2IDX[word[i + 1]]\n",
        "            A[curr_idx, next_idx] += 1\n",
        "    A = normalize_rows(A)\n",
        "\n",
        "    # Initialize emission matrix with identity + smoothing\n",
        "    # States tend to emit their corresponding letter\n",
        "    B = np.ones((n_states, n_obs)) * 0.1\n",
        "    for i in range(min(n_states, n_obs)):\n",
        "        B[i, i] = 10.0  # Strong diagonal\n",
        "    B = normalize_rows(B)\n",
        "\n",
        "    print(\"‚úÖ Smart initialization using corpus statistics\")\n",
        "    print(f\"   Top initial letters: {[(ALPHABET[i], first_letters.get(ALPHABET[i], 0)) for i in range(5)]}\")\n",
        "\n",
        "    return pi, A, B\n",
        "\n",
        "\n",
        "# ========== FORWARD-BACKWARD WITH STABILITY ==========\n",
        "def forward_backward_single(obs, pi, A, B):\n",
        "    \"\"\"\n",
        "    Scaled forward-backward algorithm with numerical stability\n",
        "    \"\"\"\n",
        "    T = len(obs)\n",
        "    N = len(pi)\n",
        "\n",
        "    alpha = np.zeros((T, N))\n",
        "    beta = np.zeros((T, N))\n",
        "    c = np.zeros(T)\n",
        "\n",
        "    # Forward pass with scaling\n",
        "    alpha[0] = pi * B[:, obs[0]]\n",
        "    c[0] = np.sum(alpha[0])\n",
        "    if c[0] > 0:\n",
        "        alpha[0] /= c[0]\n",
        "    else:\n",
        "        c[0] = 1.0\n",
        "\n",
        "    for t in range(1, T):\n",
        "        alpha[t] = (alpha[t-1] @ A) * B[:, obs[t]]\n",
        "        c[t] = np.sum(alpha[t])\n",
        "        if c[t] > 0:\n",
        "            alpha[t] /= c[t]\n",
        "        else:\n",
        "            c[t] = 1.0\n",
        "\n",
        "    # Backward pass with scaling\n",
        "    beta[-1] = 1.0\n",
        "    for t in range(T - 2, -1, -1):\n",
        "        beta[t] = A @ (B[:, obs[t + 1]] * beta[t + 1])\n",
        "        if c[t] > 0:\n",
        "            beta[t] /= c[t]\n",
        "\n",
        "    # Compute gamma (state posteriors)\n",
        "    gamma = alpha * beta\n",
        "    gamma_sum = np.sum(gamma, axis=1, keepdims=True)\n",
        "    gamma = np.where(gamma_sum > 0, gamma / gamma_sum, 1.0 / N)\n",
        "\n",
        "    # Compute xi (transition posteriors)\n",
        "    xi = np.zeros((T - 1, N, N))\n",
        "    for t in range(T - 1):\n",
        "        x = (alpha[t][:, None] * A) * (B[:, obs[t + 1]] * beta[t + 1])[None, :]\n",
        "        xi_sum = np.sum(x)\n",
        "        if xi_sum > 0:\n",
        "            xi[t] = x / xi_sum\n",
        "        else:\n",
        "            xi[t] = A / N  # Fallback\n",
        "\n",
        "    # Log-likelihood\n",
        "    log_likelihood = np.sum(np.log(c + 1e-300))\n",
        "\n",
        "    return gamma, xi, log_likelihood\n",
        "\n",
        "\n",
        "# ========== EM ALGORITHM ==========\n",
        "def estep(obs_seqs, pi, A, B, verbose=False):\n",
        "    \"\"\"E-step: compute expected sufficient statistics\"\"\"\n",
        "    N, M = len(pi), B.shape[1]\n",
        "    pi_exp = np.zeros(N)\n",
        "    A_exp = np.zeros((N, N))\n",
        "    B_exp = np.zeros((N, M))\n",
        "    total_ll = 0\n",
        "\n",
        "    for idx, obs in enumerate(obs_seqs):\n",
        "        gamma, xi, ll = forward_backward_single(obs, pi, A, B)\n",
        "        total_ll += ll\n",
        "\n",
        "        pi_exp += gamma[0]\n",
        "        A_exp += np.sum(xi, axis=0)\n",
        "\n",
        "        for t, o in enumerate(obs):\n",
        "            B_exp[:, o] += gamma[t]\n",
        "\n",
        "        if verbose and (idx + 1) % 1000 == 0:\n",
        "            print(f\"   Processed {idx + 1}/{len(obs_seqs)} sequences...\")\n",
        "\n",
        "    return pi_exp, A_exp, B_exp, total_ll\n",
        "\n",
        "\n",
        "def mstep(pi_exp, A_exp, B_exp, smoothing=1e-6):\n",
        "    \"\"\"M-step: update parameters with smoothing\"\"\"\n",
        "    pi_new = normalize_vec(pi_exp + smoothing)\n",
        "    A_new = normalize_rows(A_exp + smoothing)\n",
        "    B_new = normalize_rows(B_exp + smoothing)\n",
        "    return pi_new, A_new, B_new\n",
        "\n",
        "\n",
        "def train_hmm_baum_welch(obs_seqs, words, n_states=26, n_obs=26,\n",
        "                         max_iters=20, convergence_tol=1.0, verbose=True):\n",
        "    \"\"\"\n",
        "    Train HMM using Baum-Welch with early stopping\n",
        "\n",
        "    Args:\n",
        "        obs_seqs: Observation sequences\n",
        "        words: Original words (for smart init)\n",
        "        n_states: Number of hidden states\n",
        "        n_obs: Number of observation symbols\n",
        "        max_iters: Maximum iterations\n",
        "        convergence_tol: Stop if LL improvement < this value\n",
        "        verbose: Print progress\n",
        "    \"\"\"\n",
        "    # Smart initialization\n",
        "    pi, A, B = init_params_smart(words, n_states, n_obs)\n",
        "\n",
        "    prev_ll = None\n",
        "    best_ll = -np.inf\n",
        "    best_params = None\n",
        "    no_improvement = 0\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"üöÄ TRAINING HMM: {len(obs_seqs)} sequences, {max_iters} max iterations\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        print(f\"\\nüìä Iteration {i+1}/{max_iters}\")\n",
        "\n",
        "        # E-step\n",
        "        pi_exp, A_exp, B_exp, total_ll = estep(obs_seqs, pi, A, B, verbose=verbose)\n",
        "\n",
        "        # M-step\n",
        "        pi, A, B = mstep(pi_exp, A_exp, B_exp)\n",
        "\n",
        "        # Compute per-word average\n",
        "        avg_ll = total_ll / len(obs_seqs)\n",
        "\n",
        "        print(f\"   Total LL: {total_ll:.2f}\")\n",
        "        print(f\"   Avg LL per word: {avg_ll:.4f}\")\n",
        "\n",
        "        if prev_ll is not None:\n",
        "            improvement = total_ll - prev_ll\n",
        "            print(f\"   Improvement: {improvement:.2f}\")\n",
        "\n",
        "            # Check for convergence\n",
        "            if improvement < convergence_tol:\n",
        "                no_improvement += 1\n",
        "                print(f\"   ‚ö†Ô∏è  Small improvement ({no_improvement}/3)\")\n",
        "                if no_improvement >= 3:\n",
        "                    print(f\"\\n‚úÖ CONVERGED after {i+1} iterations!\")\n",
        "                    break\n",
        "            else:\n",
        "                no_improvement = 0\n",
        "\n",
        "        # Track best model\n",
        "        if total_ll > best_ll:\n",
        "            best_ll = total_ll\n",
        "            best_params = (pi.copy(), A.copy(), B.copy())\n",
        "            print(f\"   ‚≠ê New best model!\")\n",
        "\n",
        "        prev_ll = total_ll\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"‚úÖ TRAINING COMPLETE\")\n",
        "    print(f\"   Final LL: {prev_ll:.2f}\")\n",
        "    print(f\"   Best LL: {best_ll:.2f}\")\n",
        "    print(f\"{'='*50}\\n\")\n",
        "\n",
        "    # Return best parameters\n",
        "    return best_params if best_params else (pi, A, B)\n",
        "\n",
        "\n",
        "# ========== MAIN TRAINING PIPELINE ==========\n",
        "def main():\n",
        "    # Load corpus\n",
        "    words = load_corpus(\"corpus.txt\")\n",
        "    obs_seqs = words_to_observation_sequences(words)\n",
        "\n",
        "    print(f\"\\nüìà Total sequences: {len(obs_seqs)}\")\n",
        "\n",
        "    # Use more data for better learning (5000+ words recommended)\n",
        "    # Remove slice or increase for full corpus\n",
        "    train_size = min(5000, len(obs_seqs))\n",
        "    print(f\"   Training on: {train_size} words\")\n",
        "\n",
        "    train_words = words[:train_size]\n",
        "    train_obs = obs_seqs[:train_size]\n",
        "\n",
        "    # Train model\n",
        "    pi, A, B = train_hmm_baum_welch(\n",
        "        train_obs,\n",
        "        train_words,\n",
        "        n_states=26,\n",
        "        n_obs=26,\n",
        "        max_iters=20,\n",
        "        convergence_tol=1.0,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # Save model\n",
        "    model_data = {\n",
        "        \"pi\": pi,\n",
        "        \"A\": A,\n",
        "        \"B\": B,\n",
        "        \"train_size\": train_size,\n",
        "        \"alphabet\": ALPHABET,\n",
        "        \"sym2idx\": SYM2IDX,\n",
        "        \"idx2sym\": IDX2SYM\n",
        "    }\n",
        "\n",
        "    with open(\"hmm_trained_v1.pkl\", \"wb\") as f:\n",
        "        pickle.dump(model_data, f)\n",
        "\n",
        "    print(\"üì¶ Model saved as hmm_trained_v1.pkl\")\n",
        "\n",
        "    # Print model insights\n",
        "    print(\"\\nüîç MODEL INSIGHTS:\")\n",
        "    print(f\"   Most likely initial states: {[ALPHABET[i] for i in np.argsort(pi)[-5:][::-1]]}\")\n",
        "    print(f\"   Emission matrix sparsity: {np.sum(B < 0.01) / B.size * 100:.1f}%\")\n",
        "\n",
        "    return pi, A, B\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import string\n",
        "import pickle\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# =============================\n",
        "# HMM ACCURACY TESTING\n",
        "# =============================\n",
        "\n",
        "ALPHABET = list(string.ascii_uppercase)\n",
        "SYM2IDX = {c: i for i, c in enumerate(ALPHABET)}\n",
        "IDX2SYM = {i: c for c, i in SYM2IDX.items()}\n",
        "\n",
        "\n",
        "# ========== LOAD MODEL ==========\n",
        "def load_hmm_model(path=\"hmm_trained_v1.pkl\"):\n",
        "    \"\"\"Load trained HMM model\"\"\"\n",
        "    with open(path, \"rb\") as f:\n",
        "        model = pickle.load(f)\n",
        "    print(f\"‚úÖ Loaded HMM model from {path}\")\n",
        "    print(f\"   Training size: {model.get('train_size', 'unknown')}\")\n",
        "    return model['pi'], model['A'], model['B']\n",
        "\n",
        "\n",
        "# ========== LOAD TEST DATA ==========\n",
        "def load_test_corpus(path):\n",
        "    \"\"\"Load test corpus\"\"\"\n",
        "    words = []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            w = line.strip().upper()\n",
        "            w = ''.join([c for c in w if c in SYM2IDX])\n",
        "            if len(w) > 0:\n",
        "                words.append(w)\n",
        "\n",
        "    print(f\"‚úÖ Loaded {len(words)} test words from {path}\")\n",
        "    return words\n",
        "\n",
        "\n",
        "# ========== VITERBI ALGORITHM ==========\n",
        "def viterbi(obs, pi, A, B):\n",
        "    \"\"\"\n",
        "    Viterbi algorithm to find most likely hidden state sequence\n",
        "    Returns: best state sequence and log probability\n",
        "    \"\"\"\n",
        "    T = len(obs)\n",
        "    N = len(pi)\n",
        "\n",
        "    # Initialize\n",
        "    delta = np.zeros((T, N))\n",
        "    psi = np.zeros((T, N), dtype=int)\n",
        "\n",
        "    # t=0\n",
        "    delta[0] = np.log(pi + 1e-300) + np.log(B[:, obs[0]] + 1e-300)\n",
        "\n",
        "    # Forward pass\n",
        "    for t in range(1, T):\n",
        "        for j in range(N):\n",
        "            # Find max probability transition to state j\n",
        "            prob = delta[t-1] + np.log(A[:, j] + 1e-300)\n",
        "            psi[t, j] = np.argmax(prob)\n",
        "            delta[t, j] = prob[psi[t, j]] + np.log(B[j, obs[t]] + 1e-300)\n",
        "\n",
        "    # Backtrack\n",
        "    states = np.zeros(T, dtype=int)\n",
        "    states[-1] = np.argmax(delta[-1])\n",
        "    for t in range(T-2, -1, -1):\n",
        "        states[t] = psi[t+1, states[t+1]]\n",
        "\n",
        "    log_prob = np.max(delta[-1])\n",
        "\n",
        "    return states, log_prob\n",
        "\n",
        "\n",
        "# ========== FORWARD ALGORITHM ==========\n",
        "def forward_log_likelihood(obs, pi, A, B):\n",
        "    \"\"\"\n",
        "    Compute log-likelihood of observation sequence using forward algorithm\n",
        "    \"\"\"\n",
        "    T = len(obs)\n",
        "    N = len(pi)\n",
        "\n",
        "    alpha = np.zeros((T, N))\n",
        "    c = np.zeros(T)\n",
        "\n",
        "    # Initialize\n",
        "    alpha[0] = pi * B[:, obs[0]]\n",
        "    c[0] = np.sum(alpha[0])\n",
        "    if c[0] > 0:\n",
        "        alpha[0] /= c[0]\n",
        "    else:\n",
        "        c[0] = 1.0\n",
        "\n",
        "    # Forward pass with scaling\n",
        "    for t in range(1, T):\n",
        "        alpha[t] = (alpha[t-1] @ A) * B[:, obs[t]]\n",
        "        c[t] = np.sum(alpha[t])\n",
        "        if c[t] > 0:\n",
        "            alpha[t] /= c[t]\n",
        "        else:\n",
        "            c[t] = 1.0\n",
        "\n",
        "    # Log-likelihood\n",
        "    log_likelihood = np.sum(np.log(c + 1e-300))\n",
        "\n",
        "    return log_likelihood\n",
        "\n",
        "\n",
        "# ========== PREDICTION ACCURACY ==========\n",
        "def predict_next_letter(prefix, pi, A, B, top_k=5):\n",
        "    \"\"\"\n",
        "    Given a prefix, predict the most likely next letters\n",
        "    Returns: list of (letter, probability) tuples\n",
        "    \"\"\"\n",
        "    if len(prefix) == 0:\n",
        "        # Use initial distribution and emission\n",
        "        probs = pi @ B\n",
        "    else:\n",
        "        # Convert prefix to observations\n",
        "        obs = [SYM2IDX[c] for c in prefix]\n",
        "\n",
        "        # Run forward algorithm to get state distribution at end\n",
        "        T = len(obs)\n",
        "        N = len(pi)\n",
        "        alpha = np.zeros((T, N))\n",
        "\n",
        "        alpha[0] = pi * B[:, obs[0]]\n",
        "        alpha[0] /= np.sum(alpha[0])\n",
        "\n",
        "        for t in range(1, T):\n",
        "            alpha[t] = (alpha[t-1] @ A) * B[:, obs[t]]\n",
        "            alpha[t] /= np.sum(alpha[t])\n",
        "\n",
        "        # Predict next letter\n",
        "        state_dist = alpha[-1] @ A  # Next state distribution\n",
        "        probs = state_dist @ B  # Next letter probabilities\n",
        "\n",
        "    # Get top k predictions\n",
        "    top_indices = np.argsort(probs)[-top_k:][::-1]\n",
        "    predictions = [(ALPHABET[i], probs[i]) for i in top_indices]\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def test_next_letter_prediction(words, pi, A, B, sample_size=1000):\n",
        "    \"\"\"\n",
        "    Test accuracy of predicting next letter given prefix\n",
        "    Tests at different prefix lengths\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üéØ NEXT-LETTER PREDICTION ACCURACY\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    np.random.seed(42)\n",
        "    test_words = np.random.choice(words, min(sample_size, len(words)), replace=False)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for prefix_len in [1, 2, 3, 4, 5]:\n",
        "        correct_top1 = 0\n",
        "        correct_top3 = 0\n",
        "        correct_top5 = 0\n",
        "        total = 0\n",
        "\n",
        "        for word in test_words:\n",
        "            if len(word) <= prefix_len:\n",
        "                continue\n",
        "\n",
        "            prefix = word[:prefix_len]\n",
        "            true_next = word[prefix_len]\n",
        "\n",
        "            predictions = predict_next_letter(prefix, pi, A, B, top_k=5)\n",
        "            pred_letters = [p[0] for p in predictions]\n",
        "\n",
        "            if pred_letters[0] == true_next:\n",
        "                correct_top1 += 1\n",
        "            if true_next in pred_letters[:3]:\n",
        "                correct_top3 += 1\n",
        "            if true_next in pred_letters[:5]:\n",
        "                correct_top5 += 1\n",
        "\n",
        "            total += 1\n",
        "\n",
        "        if total > 0:\n",
        "            acc_top1 = correct_top1 / total * 100\n",
        "            acc_top3 = correct_top3 / total * 100\n",
        "            acc_top5 = correct_top5 / total * 100\n",
        "\n",
        "            results[prefix_len] = {\n",
        "                'top1': acc_top1,\n",
        "                'top3': acc_top3,\n",
        "                'top5': acc_top5,\n",
        "                'total': total\n",
        "            }\n",
        "\n",
        "            print(f\"\\nüìè Prefix length {prefix_len} ({total} samples):\")\n",
        "            print(f\"   Top-1 accuracy: {acc_top1:.2f}%\")\n",
        "            print(f\"   Top-3 accuracy: {acc_top3:.2f}%\")\n",
        "            print(f\"   Top-5 accuracy: {acc_top5:.2f}%\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ========== PERPLEXITY ==========\n",
        "def calculate_perplexity(words, pi, A, B):\n",
        "    \"\"\"\n",
        "    Calculate perplexity - lower is better\n",
        "    Perplexity = exp(-avg log likelihood)\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üìä PERPLEXITY CALCULATION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    total_ll = 0\n",
        "    total_letters = 0\n",
        "\n",
        "    for word in words:\n",
        "        obs = [SYM2IDX[c] for c in word]\n",
        "        ll = forward_log_likelihood(obs, pi, A, B)\n",
        "        total_ll += ll\n",
        "        total_letters += len(word)\n",
        "\n",
        "    avg_ll = total_ll / total_letters\n",
        "    perplexity = np.exp(-avg_ll)\n",
        "\n",
        "    print(f\"\\n   Total log-likelihood: {total_ll:.2f}\")\n",
        "    print(f\"   Avg LL per letter: {avg_ll:.4f}\")\n",
        "    print(f\"   Perplexity: {perplexity:.2f}\")\n",
        "    print(f\"   (Lower perplexity = better model)\")\n",
        "\n",
        "    return perplexity, avg_ll\n",
        "\n",
        "\n",
        "# ========== HANGMAN SIMULATION ==========\n",
        "def simulate_hangman_guess(word, pi, A, B, known_letters=set()):\n",
        "    \"\"\"\n",
        "    Simulate guessing letters for hangman\n",
        "    Returns best letter guess given current knowledge\n",
        "    \"\"\"\n",
        "    # Create pattern (known letters visible, others hidden)\n",
        "    pattern = ['_' if c not in known_letters else c for c in word]\n",
        "\n",
        "    # Score each possible letter\n",
        "    letter_scores = {}\n",
        "\n",
        "    for letter in ALPHABET:\n",
        "        if letter in known_letters:\n",
        "            continue\n",
        "\n",
        "        # Try this letter in each position\n",
        "        score = 0\n",
        "        for pos, p in enumerate(pattern):\n",
        "            if p == '_':\n",
        "                # Predict letter at this position\n",
        "                prefix = ''.join([c for c in pattern[:pos] if c != '_'])\n",
        "                predictions = predict_next_letter(prefix, pi, A, B, top_k=26)\n",
        "\n",
        "                # Find this letter's rank\n",
        "                for rank, (pred_letter, prob) in enumerate(predictions):\n",
        "                    if pred_letter == letter:\n",
        "                        score += prob\n",
        "                        break\n",
        "\n",
        "        letter_scores[letter] = score\n",
        "\n",
        "    # Return best guess\n",
        "    if letter_scores:\n",
        "        best_letter = max(letter_scores, key=letter_scores.get)\n",
        "        return best_letter, letter_scores[best_letter]\n",
        "    return None, 0\n",
        "\n",
        "\n",
        "def test_hangman_performance(words, pi, A, B, sample_size=100, max_guesses=10):\n",
        "    \"\"\"\n",
        "    Test how well the model performs at hangman\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üéÆ HANGMAN SIMULATION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    np.random.seed(42)\n",
        "    test_words = np.random.choice(words, min(sample_size, len(words)), replace=False)\n",
        "\n",
        "    total_correct = 0\n",
        "    total_guesses = 0\n",
        "    win_count = 0\n",
        "\n",
        "    for word in test_words:\n",
        "        known = set()\n",
        "        guesses = 0\n",
        "\n",
        "        while guesses < max_guesses:\n",
        "            if all(c in known for c in word):\n",
        "                win_count += 1\n",
        "                break\n",
        "\n",
        "            guess, score = simulate_hangman_guess(word, pi, A, B, known)\n",
        "            if guess is None:\n",
        "                break\n",
        "\n",
        "            guesses += 1\n",
        "            known.add(guess)\n",
        "\n",
        "            if guess in word:\n",
        "                total_correct += 1\n",
        "\n",
        "            total_guesses += 1\n",
        "\n",
        "    accuracy = total_correct / total_guesses * 100 if total_guesses > 0 else 0\n",
        "    win_rate = win_count / len(test_words) * 100\n",
        "\n",
        "    print(f\"\\n   Games played: {len(test_words)}\")\n",
        "    print(f\"   Win rate: {win_rate:.2f}%\")\n",
        "    print(f\"   Letter guess accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"   Avg guesses per game: {total_guesses / len(test_words):.2f}\")\n",
        "\n",
        "    return win_rate, accuracy\n",
        "\n",
        "\n",
        "# ========== MAIN TESTING ==========\n",
        "def main():\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üß™ HMM MODEL ACCURACY TESTING\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load model\n",
        "    pi, A, B = load_hmm_model(\"hmm_trained.pkl\")\n",
        "\n",
        "    # Load test data\n",
        "    test_words = load_test_corpus(\"test.txt\")\n",
        "\n",
        "    print(f\"\\nüìã Test dataset: {len(test_words)} words\")\n",
        "\n",
        "    # Test 1: Perplexity (overall fit)\n",
        "    perplexity, avg_ll = calculate_perplexity(test_words, pi, A, B)\n",
        "\n",
        "    # Test 2: Next-letter prediction accuracy\n",
        "    pred_results = test_next_letter_prediction(test_words, pi, A, B, sample_size=2000)\n",
        "\n",
        "    # Test 3: Hangman simulation\n",
        "    win_rate, guess_acc = test_hangman_performance(test_words, pi, A, B, sample_size=200)\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üìà SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\n‚úÖ Perplexity: {perplexity:.2f} (lower is better)\")\n",
        "    print(f\"‚úÖ Avg log-likelihood per letter: {avg_ll:.4f}\")\n",
        "\n",
        "    if pred_results:\n",
        "        avg_top1 = np.mean([r['top1'] for r in pred_results.values()])\n",
        "        avg_top3 = np.mean([r['top3'] for r in pred_results.values()])\n",
        "        print(f\"‚úÖ Avg Top-1 prediction accuracy: {avg_top1:.2f}%\")\n",
        "        print(f\"‚úÖ Avg Top-3 prediction accuracy: {avg_top3:.2f}%\")\n",
        "\n",
        "    print(f\"‚úÖ Hangman win rate: {win_rate:.2f}%\")\n",
        "    print(f\"‚úÖ Hangman guess accuracy: {guess_acc:.2f}%\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GryufKtW2O-i",
        "outputId": "bd385857-d7dd-4c36-8be4-185604d820b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "üß™ HMM MODEL ACCURACY TESTING\n",
            "============================================================\n",
            "‚úÖ Loaded HMM model from hmm_trained.pkl\n",
            "   Training size: 5000\n",
            "‚úÖ Loaded 2000 test words from test.txt\n",
            "\n",
            "üìã Test dataset: 2000 words\n",
            "\n",
            "============================================================\n",
            "üìä PERPLEXITY CALCULATION\n",
            "============================================================\n",
            "\n",
            "   Total log-likelihood: -48215.19\n",
            "   Avg LL per letter: -2.5009\n",
            "   Perplexity: 12.19\n",
            "   (Lower perplexity = better model)\n",
            "\n",
            "============================================================\n",
            "üéØ NEXT-LETTER PREDICTION ACCURACY\n",
            "============================================================\n",
            "\n",
            "üìè Prefix length 1 (2000 samples):\n",
            "   Top-1 accuracy: 26.80%\n",
            "   Top-3 accuracy: 56.55%\n",
            "   Top-5 accuracy: 76.20%\n",
            "\n",
            "üìè Prefix length 2 (1998 samples):\n",
            "   Top-1 accuracy: 16.27%\n",
            "   Top-3 accuracy: 38.29%\n",
            "   Top-5 accuracy: 56.76%\n",
            "\n",
            "üìè Prefix length 3 (1989 samples):\n",
            "   Top-1 accuracy: 17.65%\n",
            "   Top-3 accuracy: 39.77%\n",
            "   Top-5 accuracy: 56.26%\n",
            "\n",
            "üìè Prefix length 4 (1952 samples):\n",
            "   Top-1 accuracy: 19.93%\n",
            "   Top-3 accuracy: 45.75%\n",
            "   Top-5 accuracy: 61.78%\n",
            "\n",
            "üìè Prefix length 5 (1861 samples):\n",
            "   Top-1 accuracy: 18.75%\n",
            "   Top-3 accuracy: 45.19%\n",
            "   Top-5 accuracy: 62.92%\n",
            "\n",
            "============================================================\n",
            "üéÆ HANGMAN SIMULATION\n",
            "============================================================\n",
            "\n",
            "   Games played: 200\n",
            "   Win rate: 3.00%\n",
            "   Letter guess accuracy: 48.46%\n",
            "   Avg guesses per game: 9.93\n",
            "\n",
            "============================================================\n",
            "üìà SUMMARY\n",
            "============================================================\n",
            "\n",
            "‚úÖ Perplexity: 12.19 (lower is better)\n",
            "‚úÖ Avg log-likelihood per letter: -2.5009\n",
            "‚úÖ Avg Top-1 prediction accuracy: 19.88%\n",
            "‚úÖ Avg Top-3 prediction accuracy: 45.11%\n",
            "‚úÖ Hangman win rate: 3.00%\n",
            "‚úÖ Hangman guess accuracy: 48.46%\n",
            "\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import string\n",
        "import pickle\n",
        "from collections import Counter, defaultdict\n",
        "import random\n",
        "\n",
        "ALPHABET = list(string.ascii_uppercase)\n",
        "SYM2IDX = {c: i for i, c in enumerate(ALPHABET)}\n",
        "IDX2SYM = {i: c for c, i in SYM2IDX.items()}\n",
        "\n",
        "\n",
        "class HMMFeatureExtractor:\n",
        "    \"\"\"\n",
        "    Extracts HMM-based features for RL state representation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pi, A, B):\n",
        "        self.pi = pi\n",
        "        self.A = A\n",
        "        self.B = B\n",
        "        self.N = len(pi)\n",
        "\n",
        "    def get_letter_probabilities(self, pattern, guessed_letters):\n",
        "        \"\"\"\n",
        "        Get HMM probability distribution over all 26 letters\n",
        "        Returns: vector of 26 probabilities\n",
        "        \"\"\"\n",
        "        length = len(pattern)\n",
        "\n",
        "        # Convert pattern to observation indices (-1 for unknown)\n",
        "        obs = []\n",
        "        unknown_positions = []\n",
        "        for i, c in enumerate(pattern):\n",
        "            if c == '_':\n",
        "                obs.append(-1)\n",
        "                unknown_positions.append(i)\n",
        "            else:\n",
        "                obs.append(SYM2IDX[c])\n",
        "\n",
        "        if not unknown_positions:\n",
        "            return np.zeros(26)\n",
        "\n",
        "        # Run forward-backward\n",
        "        alpha = self._forward(obs)\n",
        "        beta = self._backward(obs)\n",
        "\n",
        "        # Compute marginal probabilities for each letter\n",
        "        letter_probs = np.zeros(26)\n",
        "\n",
        "        for pos in unknown_positions:\n",
        "            gamma = alpha[pos] * beta[pos]\n",
        "            gamma = gamma / (np.sum(gamma) + 1e-300)\n",
        "\n",
        "            # Accumulate emission probabilities\n",
        "            for letter_idx in range(26):\n",
        "                letter_probs[letter_idx] += np.sum(gamma * self.B[:, letter_idx])\n",
        "\n",
        "        # Normalize\n",
        "        total = np.sum(letter_probs)\n",
        "        if total > 0:\n",
        "            letter_probs = letter_probs / total\n",
        "\n",
        "        # Zero out already guessed letters\n",
        "        for letter in guessed_letters:\n",
        "            letter_probs[SYM2IDX[letter]] = 0\n",
        "\n",
        "        return letter_probs\n",
        "\n",
        "    def get_state_distribution(self, pattern):\n",
        "        \"\"\"\n",
        "        Get HMM hidden state distribution given pattern\n",
        "        Returns: vector of state probabilities\n",
        "        \"\"\"\n",
        "        obs = []\n",
        "        for c in pattern:\n",
        "            if c == '_':\n",
        "                obs.append(-1)\n",
        "            else:\n",
        "                obs.append(SYM2IDX[c])\n",
        "\n",
        "        alpha = self._forward(obs)\n",
        "        return alpha[-1] / (np.sum(alpha[-1]) + 1e-300)\n",
        "\n",
        "    def _forward(self, obs):\n",
        "        \"\"\"Forward algorithm with unknown observations\"\"\"\n",
        "        T = len(obs)\n",
        "        alpha = np.zeros((T, self.N))\n",
        "\n",
        "        # Initialize\n",
        "        if obs[0] == -1:\n",
        "            alpha[0] = self.pi\n",
        "        else:\n",
        "            alpha[0] = self.pi * self.B[:, obs[0]]\n",
        "        alpha[0] = alpha[0] / (np.sum(alpha[0]) + 1e-300)\n",
        "\n",
        "        # Forward pass\n",
        "        for t in range(1, T):\n",
        "            alpha[t] = alpha[t-1] @ self.A\n",
        "            if obs[t] != -1:\n",
        "                alpha[t] = alpha[t] * self.B[:, obs[t]]\n",
        "            alpha[t] = alpha[t] / (np.sum(alpha[t]) + 1e-300)\n",
        "\n",
        "        return alpha\n",
        "\n",
        "    def _backward(self, obs):\n",
        "        \"\"\"Backward algorithm with unknown observations\"\"\"\n",
        "        T = len(obs)\n",
        "        beta = np.zeros((T, self.N))\n",
        "        beta[-1] = 1.0\n",
        "\n",
        "        for t in range(T-2, -1, -1):\n",
        "            if obs[t+1] != -1:\n",
        "                beta[t] = self.A @ (self.B[:, obs[t+1]] * beta[t+1])\n",
        "            else:\n",
        "                beta[t] = self.A @ beta[t+1]\n",
        "            beta[t] = beta[t] / (np.sum(beta[t]) + 1e-300)\n",
        "\n",
        "        return beta\n",
        "\n",
        "\n",
        "class HangmanQLearningAgent:\n",
        "    \"\"\"\n",
        "    Q-Learning agent for Hangman that uses HMM features\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hmm_extractor, learning_rate=0.1, discount=0.95,\n",
        "                 epsilon=0.1, epsilon_decay=0.995, epsilon_min=0.01):\n",
        "        self.hmm = hmm_extractor\n",
        "        self.alpha = learning_rate\n",
        "        self.gamma = discount\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "\n",
        "        # Q-table: state -> action -> value\n",
        "        self.Q = defaultdict(lambda: np.zeros(26))\n",
        "\n",
        "        # For tracking performance\n",
        "        self.training_stats = {\n",
        "            'games_played': 0,\n",
        "            'games_won': 0,\n",
        "            'total_wrong_guesses': 0,\n",
        "            'total_repeated_guesses': 0\n",
        "        }\n",
        "\n",
        "    def get_state_key(self, pattern, guessed_letters, wrong_guesses_left):\n",
        "        \"\"\"\n",
        "        Create state representation combining:\n",
        "        - Pattern (compressed)\n",
        "        - Guessed letters (as bitmask)\n",
        "        - Wrong guesses remaining\n",
        "        - HMM top predictions (discretized)\n",
        "        \"\"\"\n",
        "        # Pattern compression: length + number of known letters + positions\n",
        "        length = len(pattern)\n",
        "        known_count = sum(1 for c in pattern if c != '_')\n",
        "\n",
        "        # Get HMM probabilities\n",
        "        hmm_probs = self.hmm.get_letter_probabilities(pattern, guessed_letters)\n",
        "\n",
        "        # Get top 5 HMM predictions (discretized)\n",
        "        top_5_indices = np.argsort(hmm_probs)[-5:][::-1]\n",
        "        top_5_letters = tuple(ALPHABET[i] for i in top_5_indices)\n",
        "\n",
        "        # Guessed letters as sorted tuple\n",
        "        guessed_tuple = tuple(sorted(guessed_letters))\n",
        "\n",
        "        # Create composite state key\n",
        "        state_key = (\n",
        "            length,\n",
        "            known_count,\n",
        "            pattern,  # Full pattern for exact matching\n",
        "            guessed_tuple,\n",
        "            wrong_guesses_left,\n",
        "            top_5_letters  # HMM guidance\n",
        "        )\n",
        "\n",
        "        return state_key\n",
        "\n",
        "    def get_state_key_simple(self, pattern, guessed_letters, wrong_guesses_left):\n",
        "        \"\"\"\n",
        "        Simpler state representation for smaller state space\n",
        "        \"\"\"\n",
        "        # Get HMM probabilities\n",
        "        hmm_probs = self.hmm.get_letter_probabilities(pattern, guessed_letters)\n",
        "\n",
        "        # Discretize HMM probs into bins\n",
        "        top_3_indices = np.argsort(hmm_probs)[-3:][::-1]\n",
        "        top_3_letters = tuple(ALPHABET[i] for i in top_3_indices)\n",
        "\n",
        "        # Pattern features\n",
        "        length = len(pattern)\n",
        "        known_count = sum(1 for c in pattern if c != '_')\n",
        "        known_ratio = int(known_count / length * 10)  # 0-10\n",
        "\n",
        "        state_key = (\n",
        "            length,\n",
        "            known_ratio,\n",
        "            wrong_guesses_left,\n",
        "            top_3_letters,\n",
        "            tuple(sorted(guessed_letters))\n",
        "        )\n",
        "\n",
        "        return state_key\n",
        "\n",
        "    def choose_action(self, state_key, guessed_letters, hmm_probs, training=True):\n",
        "        \"\"\"\n",
        "        Choose action using epsilon-greedy with HMM guidance\n",
        "        \"\"\"\n",
        "        available_letters = [i for i in range(26) if ALPHABET[i] not in guessed_letters]\n",
        "\n",
        "        if not available_letters:\n",
        "            return None\n",
        "\n",
        "        # Epsilon-greedy exploration\n",
        "        if training and random.random() < self.epsilon:\n",
        "            # Explore: weighted random by HMM probs\n",
        "            probs = np.array([hmm_probs[i] for i in available_letters])\n",
        "            if np.sum(probs) > 0:\n",
        "                probs = probs / np.sum(probs)\n",
        "                action = np.random.choice(available_letters, p=probs)\n",
        "            else:\n",
        "                action = random.choice(available_letters)\n",
        "        else:\n",
        "            # Exploit: choose best Q-value among available\n",
        "            q_values = self.Q[state_key]\n",
        "\n",
        "            # Combine Q-values with HMM probs for better decisions\n",
        "            combined_scores = np.zeros(26)\n",
        "            for i in available_letters:\n",
        "                # 70% Q-value, 30% HMM probability\n",
        "                combined_scores[i] = 0.7 * q_values[i] + 0.3 * hmm_probs[i]\n",
        "\n",
        "            action = np.argmax(combined_scores)\n",
        "\n",
        "            # If Q-value is 0 (unvisited), fall back to HMM\n",
        "            if q_values[action] == 0:\n",
        "                action = np.argmax(hmm_probs)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def update_q_value(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Q-learning update: Q(s,a) += Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]\n",
        "        \"\"\"\n",
        "        current_q = self.Q[state][action]\n",
        "\n",
        "        if done:\n",
        "            max_next_q = 0\n",
        "        else:\n",
        "            max_next_q = np.max(self.Q[next_state])\n",
        "\n",
        "        # Q-learning update\n",
        "        new_q = current_q + self.alpha * (reward + self.gamma * max_next_q - current_q)\n",
        "        self.Q[state][action] = new_q\n",
        "\n",
        "    def train_episode(self, word, max_wrong=6):\n",
        "        \"\"\"\n",
        "        Train on a single game of Hangman\n",
        "        \"\"\"\n",
        "        pattern = ['_'] * len(word)\n",
        "        guessed_letters = set()\n",
        "        wrong_guesses = 0\n",
        "        repeated_guesses = 0\n",
        "\n",
        "        episode_history = []  # (state, action, reward)\n",
        "\n",
        "        while wrong_guesses < max_wrong:\n",
        "            # Check win condition\n",
        "            if '_' not in pattern:\n",
        "                # Won! Backpropagate rewards\n",
        "                final_reward = 100 - (wrong_guesses * 5) - (repeated_guesses * 2)\n",
        "                self._backpropagate_rewards(episode_history, final_reward)\n",
        "                self.training_stats['games_won'] += 1\n",
        "                return True, wrong_guesses, repeated_guesses\n",
        "\n",
        "            # Get current state\n",
        "            pattern_str = ''.join(pattern)\n",
        "            hmm_probs = self.hmm.get_letter_probabilities(pattern_str, guessed_letters)\n",
        "            state = self.get_state_key_simple(pattern_str, guessed_letters, max_wrong - wrong_guesses)\n",
        "\n",
        "            # Choose action\n",
        "            action = self.choose_action(state, guessed_letters, hmm_probs, training=True)\n",
        "            if action is None:\n",
        "                break\n",
        "\n",
        "            letter = ALPHABET[action]\n",
        "\n",
        "            # Check for repeated guess\n",
        "            if letter in guessed_letters:\n",
        "                repeated_guesses += 1\n",
        "                reward = -10  # Heavy penalty for repeated guess\n",
        "                episode_history.append((state, action, reward))\n",
        "                continue\n",
        "\n",
        "            guessed_letters.add(letter)\n",
        "\n",
        "            # Check if letter is in word\n",
        "            if letter in word:\n",
        "                # Correct guess\n",
        "                for i, c in enumerate(word):\n",
        "                    if c == letter:\n",
        "                        pattern[i] = letter\n",
        "\n",
        "                reward = 5  # Reward for correct guess\n",
        "            else:\n",
        "                # Wrong guess\n",
        "                wrong_guesses += 1\n",
        "                reward = -5  # Penalty for wrong guess\n",
        "\n",
        "            episode_history.append((state, action, reward))\n",
        "\n",
        "        # Lost game\n",
        "        final_reward = -(wrong_guesses * 5) - (repeated_guesses * 2) - 50\n",
        "        self._backpropagate_rewards(episode_history, final_reward)\n",
        "\n",
        "        return False, wrong_guesses, repeated_guesses\n",
        "\n",
        "    def _backpropagate_rewards(self, episode_history, final_reward):\n",
        "        \"\"\"\n",
        "        Backpropagate rewards through episode\n",
        "        \"\"\"\n",
        "        # Update Q-values for all state-action pairs in episode\n",
        "        for i, (state, action, immediate_reward) in enumerate(episode_history):\n",
        "            # Calculate discounted future reward\n",
        "            remaining_steps = len(episode_history) - i - 1\n",
        "            discounted_final = final_reward * (self.gamma ** remaining_steps)\n",
        "            total_reward = immediate_reward + discounted_final\n",
        "\n",
        "            # Get next state\n",
        "            if i < len(episode_history) - 1:\n",
        "                next_state = episode_history[i + 1][0]\n",
        "                done = False\n",
        "            else:\n",
        "                next_state = state\n",
        "                done = True\n",
        "\n",
        "            self.update_q_value(state, action, total_reward, next_state, done)\n",
        "\n",
        "    def train(self, word_list, episodes=5000):\n",
        "        \"\"\"\n",
        "        Train the Q-learning agent on a list of words\n",
        "        \"\"\"\n",
        "        print(f\"\\nüéì Training Q-Learning Agent for {episodes} episodes...\")\n",
        "        print(f\"   Word list size: {len(word_list)}\")\n",
        "\n",
        "        for episode in range(episodes):\n",
        "            # Sample random word\n",
        "            word = random.choice(word_list).upper()\n",
        "            word = ''.join([c for c in word if c in SYM2IDX])\n",
        "\n",
        "            if len(word) == 0:\n",
        "                continue\n",
        "\n",
        "            # Train episode\n",
        "            won, wrong, repeated = self.train_episode(word)\n",
        "\n",
        "            # Update stats\n",
        "            self.training_stats['games_played'] += 1\n",
        "            self.training_stats['total_wrong_guesses'] += wrong\n",
        "            self.training_stats['total_repeated_guesses'] += repeated\n",
        "\n",
        "            # Decay epsilon\n",
        "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "            # Print progress\n",
        "            if (episode + 1) % 500 == 0:\n",
        "                games = self.training_stats['games_played']\n",
        "                wins = self.training_stats['games_won']\n",
        "                win_rate = wins / games * 100 if games > 0 else 0\n",
        "                avg_wrong = self.training_stats['total_wrong_guesses'] / games\n",
        "                avg_repeated = self.training_stats['total_repeated_guesses'] / games\n",
        "\n",
        "                print(f\"\\n   Episode {episode + 1}/{episodes}\")\n",
        "                print(f\"   Win rate: {win_rate:.2f}%\")\n",
        "                print(f\"   Avg wrong guesses: {avg_wrong:.2f}\")\n",
        "                print(f\"   Avg repeated guesses: {avg_repeated:.2f}\")\n",
        "                print(f\"   Epsilon: {self.epsilon:.4f}\")\n",
        "                print(f\"   Q-table size: {len(self.Q)}\")\n",
        "\n",
        "        print(f\"\\n‚úÖ Training complete!\")\n",
        "        print(f\"   Final win rate: {self.training_stats['games_won'] / self.training_stats['games_played'] * 100:.2f}%\")\n",
        "        print(f\"   Q-table states: {len(self.Q)}\")\n",
        "\n",
        "    def play_game(self, word, max_wrong=6, verbose=False):\n",
        "        \"\"\"\n",
        "        Play a single game (testing/evaluation)\n",
        "        \"\"\"\n",
        "        pattern = ['_'] * len(word)\n",
        "        guessed_letters = set()\n",
        "        wrong_guesses = 0\n",
        "        repeated_guesses = 0\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nüéÆ Playing: {word}\")\n",
        "\n",
        "        while wrong_guesses < max_wrong:\n",
        "            if '_' not in pattern:\n",
        "                if verbose:\n",
        "                    print(f\"   ‚úÖ WON! Wrong: {wrong_guesses}, Repeated: {repeated_guesses}\")\n",
        "                return True, wrong_guesses, repeated_guesses\n",
        "\n",
        "            pattern_str = ''.join(pattern)\n",
        "            hmm_probs = self.hmm.get_letter_probabilities(pattern_str, guessed_letters)\n",
        "            state = self.get_state_key_simple(pattern_str, guessed_letters, max_wrong - wrong_guesses)\n",
        "\n",
        "            action = self.choose_action(state, guessed_letters, hmm_probs, training=False)\n",
        "            if action is None:\n",
        "                break\n",
        "\n",
        "            letter = ALPHABET[action]\n",
        "\n",
        "            if letter in guessed_letters:\n",
        "                repeated_guesses += 1\n",
        "                if verbose:\n",
        "                    print(f\"   ‚ùå Repeated: {letter}\")\n",
        "                continue\n",
        "\n",
        "            guessed_letters.add(letter)\n",
        "\n",
        "            if letter in word:\n",
        "                for i, c in enumerate(word):\n",
        "                    if c == letter:\n",
        "                        pattern[i] = letter\n",
        "                if verbose:\n",
        "                    print(f\"   ‚úì {letter}: {''.join(pattern)}\")\n",
        "            else:\n",
        "                wrong_guesses += 1\n",
        "                if verbose:\n",
        "                    print(f\"   ‚úó {letter}: {''.join(pattern)} ({wrong_guesses}/{max_wrong})\")\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"   ‚ùå LOST! Wrong: {wrong_guesses}, Repeated: {repeated_guesses}\")\n",
        "        return False, wrong_guesses, repeated_guesses\n",
        "\n",
        "    def evaluate(self, test_words, max_wrong=6):\n",
        "        \"\"\"\n",
        "        Evaluate agent on test set\n",
        "        \"\"\"\n",
        "        print(f\"\\nüìä Evaluating on {len(test_words)} words...\")\n",
        "\n",
        "        wins = 0\n",
        "        total_wrong = 0\n",
        "        total_repeated = 0\n",
        "\n",
        "        for word in test_words:\n",
        "            won, wrong, repeated = self.play_game(word, max_wrong)\n",
        "            if won:\n",
        "                wins += 1\n",
        "            total_wrong += wrong\n",
        "            total_repeated += repeated\n",
        "\n",
        "        success_rate = wins / len(test_words)\n",
        "        final_score = (success_rate * 2000) - (total_wrong * 5) - (total_repeated * 2)\n",
        "\n",
        "        print(f\"\\nüìà RESULTS:\")\n",
        "        print(f\"   Games played: {len(test_words)}\")\n",
        "        print(f\"   Wins: {wins}\")\n",
        "        print(f\"   Success rate: {success_rate * 100:.2f}%\")\n",
        "        print(f\"   Total wrong guesses: {total_wrong}\")\n",
        "        print(f\"   Total repeated guesses: {total_repeated}\")\n",
        "        print(f\"   Avg wrong per game: {total_wrong / len(test_words):.2f}\")\n",
        "        print(f\"   Avg repeated per game: {total_repeated / len(test_words):.2f}\")\n",
        "        print(f\"\\n   üèÜ FINAL SCORE: {final_score:.2f}\")\n",
        "\n",
        "        return success_rate, total_wrong, total_repeated, final_score\n",
        "\n",
        "    def save(self, path=\"q_learning_agent.pkl\"):\n",
        "        \"\"\"Save Q-table and parameters\"\"\"\n",
        "        data = {\n",
        "            'Q': dict(self.Q),\n",
        "            'alpha': self.alpha,\n",
        "            'gamma': self.gamma,\n",
        "            'epsilon': self.epsilon,\n",
        "            'stats': self.training_stats\n",
        "        }\n",
        "        with open(path, 'wb') as f:\n",
        "            pickle.dump(data, f)\n",
        "        print(f\"‚úÖ Saved agent to {path}\")\n",
        "\n",
        "    def load(self, path=\"q_learning_agent.pkl\"):\n",
        "        \"\"\"Load Q-table and parameters\"\"\"\n",
        "        with open(path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        self.Q = defaultdict(lambda: np.zeros(26), data['Q'])\n",
        "        self.alpha = data['alpha']\n",
        "        self.gamma = data['gamma']\n",
        "        self.epsilon = data['epsilon']\n",
        "        self.training_stats = data['stats']\n",
        "        print(f\"‚úÖ Loaded agent from {path}\")\n",
        "\n",
        "\n",
        "# ========== MAIN USAGE ==========\n",
        "def main():\n",
        "    print(\"=\"*60)\n",
        "    print(\"ü§ñ HMM + Q-Learning Hangman Agent\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load HMM model\n",
        "    print(\"\\n1Ô∏è‚É£ Loading HMM model...\")\n",
        "    with open(\"hmm_trained.pkl\", \"rb\") as f:\n",
        "        model = pickle.load(f)\n",
        "    pi, A, B = model['pi'], model['A'], model['B']\n",
        "\n",
        "    # Create HMM feature extractor\n",
        "    hmm_extractor = HMMFeatureExtractor(pi, A, B)\n",
        "\n",
        "    # Load training words\n",
        "    print(\"\\n2Ô∏è‚É£ Loading training words...\")\n",
        "    with open(\"corpus.txt\", 'r', encoding='utf-8') as f:\n",
        "        train_words = [line.strip().upper() for line in f]\n",
        "    train_words = [''.join([c for c in w if c in SYM2IDX]) for w in train_words]\n",
        "    train_words = [w for w in train_words if len(w) > 0]\n",
        "    print(f\"   Loaded {len(train_words)} training words\")\n",
        "\n",
        "    # Create Q-learning agent\n",
        "    agent = HangmanQLearningAgent(\n",
        "        hmm_extractor,\n",
        "        learning_rate=0.1,\n",
        "        discount=0.95,\n",
        "        epsilon=0.3,\n",
        "        epsilon_decay=0.995,\n",
        "        epsilon_min=0.01\n",
        "    )\n",
        "\n",
        "    # Train agent\n",
        "    print(\"\\n3Ô∏è‚É£ Training agent...\")\n",
        "    agent.train(train_words, episodes=10000)\n",
        "\n",
        "    # Save agent\n",
        "    agent.save(\"hangman_qlearning_agent.pkl\")\n",
        "\n",
        "    # Load test words\n",
        "    print(\"\\n4Ô∏è‚É£ Loading test words...\")\n",
        "    with open(\"test.txt\", 'r', encoding='utf-8') as f:\n",
        "        test_words = [line.strip().upper() for line in f]\n",
        "    test_words = [''.join([c for c in w if c in SYM2IDX]) for w in test_words]\n",
        "    test_words = [w for w in test_words if len(w) > 0]\n",
        "    print(f\"   Loaded {len(test_words)} test words\")\n",
        "\n",
        "    # Evaluate\n",
        "    print(\"\\n5Ô∏è‚É£ Evaluating agent...\")\n",
        "    agent.evaluate(test_words[:2000], max_wrong=6)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZk9S86139HL",
        "outputId": "0001cec6-6a99-4653-a864-71654743b542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ü§ñ HMM + Q-Learning Hangman Agent\n",
            "============================================================\n",
            "\n",
            "1Ô∏è‚É£ Loading HMM model...\n",
            "\n",
            "2Ô∏è‚É£ Loading training words...\n",
            "   Loaded 50000 training words\n",
            "\n",
            "3Ô∏è‚É£ Training agent...\n",
            "\n",
            "üéì Training Q-Learning Agent for 10000 episodes...\n",
            "   Word list size: 50000\n",
            "\n",
            "   Episode 500/10000\n",
            "   Win rate: 27.20%\n",
            "   Avg wrong guesses: 5.38\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0245\n",
            "   Q-table size: 4670\n",
            "\n",
            "   Episode 1000/10000\n",
            "   Win rate: 29.10%\n",
            "   Avg wrong guesses: 5.35\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 8789\n",
            "\n",
            "   Episode 1500/10000\n",
            "   Win rate: 30.53%\n",
            "   Avg wrong guesses: 5.31\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 12607\n",
            "\n",
            "   Episode 2000/10000\n",
            "   Win rate: 31.65%\n",
            "   Avg wrong guesses: 5.29\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 16153\n",
            "\n",
            "   Episode 2500/10000\n",
            "   Win rate: 32.64%\n",
            "   Avg wrong guesses: 5.26\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 19601\n",
            "\n",
            "   Episode 3000/10000\n",
            "   Win rate: 32.23%\n",
            "   Avg wrong guesses: 5.26\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 22819\n",
            "\n",
            "   Episode 3500/10000\n",
            "   Win rate: 32.20%\n",
            "   Avg wrong guesses: 5.26\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 26057\n",
            "\n",
            "   Episode 4000/10000\n",
            "   Win rate: 32.12%\n",
            "   Avg wrong guesses: 5.26\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 29145\n",
            "\n",
            "   Episode 4500/10000\n",
            "   Win rate: 32.11%\n",
            "   Avg wrong guesses: 5.25\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 32287\n",
            "\n",
            "   Episode 5000/10000\n",
            "   Win rate: 32.40%\n",
            "   Avg wrong guesses: 5.25\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 35372\n",
            "\n",
            "   Episode 5500/10000\n",
            "   Win rate: 32.78%\n",
            "   Avg wrong guesses: 5.24\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 38515\n",
            "\n",
            "   Episode 6000/10000\n",
            "   Win rate: 33.32%\n",
            "   Avg wrong guesses: 5.22\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 41368\n",
            "\n",
            "   Episode 6500/10000\n",
            "   Win rate: 33.25%\n",
            "   Avg wrong guesses: 5.22\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 44277\n",
            "\n",
            "   Episode 7000/10000\n",
            "   Win rate: 33.27%\n",
            "   Avg wrong guesses: 5.22\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 47140\n",
            "\n",
            "   Episode 7500/10000\n",
            "   Win rate: 33.45%\n",
            "   Avg wrong guesses: 5.21\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 49931\n",
            "\n",
            "   Episode 8000/10000\n",
            "   Win rate: 33.46%\n",
            "   Avg wrong guesses: 5.21\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 52754\n",
            "\n",
            "   Episode 8500/10000\n",
            "   Win rate: 33.46%\n",
            "   Avg wrong guesses: 5.21\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 55446\n",
            "\n",
            "   Episode 9000/10000\n",
            "   Win rate: 33.70%\n",
            "   Avg wrong guesses: 5.20\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 58085\n",
            "\n",
            "   Episode 9500/10000\n",
            "   Win rate: 34.02%\n",
            "   Avg wrong guesses: 5.20\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 60850\n",
            "\n",
            "   Episode 10000/10000\n",
            "   Win rate: 34.03%\n",
            "   Avg wrong guesses: 5.20\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 63411\n",
            "\n",
            "‚úÖ Training complete!\n",
            "   Final win rate: 34.03%\n",
            "   Q-table states: 63411\n",
            "‚úÖ Saved agent to hangman_qlearning_agent.pkl\n",
            "\n",
            "4Ô∏è‚É£ Loading test words...\n",
            "   Loaded 2000 test words\n",
            "\n",
            "5Ô∏è‚É£ Evaluating agent...\n",
            "\n",
            "üìä Evaluating on 2000 words...\n",
            "\n",
            "üìà RESULTS:\n",
            "   Games played: 2000\n",
            "   Wins: 698\n",
            "   Success rate: 34.90%\n",
            "   Total wrong guesses: 10286\n",
            "   Total repeated guesses: 0\n",
            "   Avg wrong per game: 5.14\n",
            "   Avg repeated per game: 0.00\n",
            "\n",
            "   üèÜ FINAL SCORE: -50732.00\n",
            "\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import string\n",
        "import pickle\n",
        "from collections import deque\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "ALPHABET = list(string.ascii_uppercase)\n",
        "SYM2IDX = {c: i for i, c in enumerate(ALPHABET)}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üöÄ Device: {device}\")\n",
        "\n",
        "\n",
        "class SimpleHMM:\n",
        "    \"\"\"Simple cached HMM\"\"\"\n",
        "\n",
        "    def __init__(self, pi, A, B):\n",
        "        self.pi = pi\n",
        "        self.A = A\n",
        "        self.B = B\n",
        "        self.N = len(pi)\n",
        "        self.cache = {}\n",
        "\n",
        "    def get_probs(self, pattern, guessed):\n",
        "        \"\"\"Get letter probabilities\"\"\"\n",
        "        key = (pattern, tuple(sorted(guessed)))\n",
        "        if key in self.cache:\n",
        "            return self.cache[key]\n",
        "\n",
        "        # Parse pattern\n",
        "        obs = []\n",
        "        unknowns = []\n",
        "        for i, c in enumerate(pattern):\n",
        "            if c == '_':\n",
        "                obs.append(-1)\n",
        "                unknowns.append(i)\n",
        "            else:\n",
        "                obs.append(SYM2IDX[c])\n",
        "\n",
        "        if not unknowns:\n",
        "            result = np.zeros(26)\n",
        "            self.cache[key] = result\n",
        "            return result\n",
        "\n",
        "        # Forward-backward\n",
        "        alpha = self._forward(obs)\n",
        "        beta = self._backward(obs)\n",
        "\n",
        "        probs = np.zeros(26)\n",
        "        for pos in unknowns:\n",
        "            gamma = alpha[pos] * beta[pos]\n",
        "            gamma = gamma / (gamma.sum() + 1e-10)\n",
        "            probs += gamma @ self.B\n",
        "\n",
        "        probs = probs / (probs.sum() + 1e-10)\n",
        "\n",
        "        # Zero guessed\n",
        "        for letter in guessed:\n",
        "            probs[SYM2IDX[letter]] = 0\n",
        "\n",
        "        # Renormalize\n",
        "        if probs.sum() > 0:\n",
        "            probs = probs / probs.sum()\n",
        "\n",
        "        self.cache[key] = probs\n",
        "        return probs\n",
        "\n",
        "    def _forward(self, obs):\n",
        "        T = len(obs)\n",
        "        alpha = np.zeros((T, self.N))\n",
        "\n",
        "        if obs[0] == -1:\n",
        "            alpha[0] = self.pi\n",
        "        else:\n",
        "            alpha[0] = self.pi * self.B[:, obs[0]]\n",
        "        alpha[0] = alpha[0] / (alpha[0].sum() + 1e-10)\n",
        "\n",
        "        for t in range(1, T):\n",
        "            alpha[t] = alpha[t-1] @ self.A\n",
        "            if obs[t] != -1:\n",
        "                alpha[t] = alpha[t] * self.B[:, obs[t]]\n",
        "            alpha[t] = alpha[t] / (alpha[t].sum() + 1e-10)\n",
        "\n",
        "        return alpha\n",
        "\n",
        "    def _backward(self, obs):\n",
        "        T = len(obs)\n",
        "        beta = np.zeros((T, self.N))\n",
        "        beta[-1] = 1.0\n",
        "\n",
        "        for t in range(T-2, -1, -1):\n",
        "            if obs[t+1] != -1:\n",
        "                beta[t] = self.A @ (self.B[:, obs[t+1]] * beta[t+1])\n",
        "            else:\n",
        "                beta[t] = self.A @ beta[t+1]\n",
        "            beta[t] = beta[t] / (beta[t].sum() + 1e-10)\n",
        "\n",
        "        return beta\n",
        "\n",
        "\n",
        "class StableDQN(nn.Module):\n",
        "    \"\"\"Stable DQN with proper initialization\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Smaller network for stability\n",
        "        self.fc1 = nn.Linear(52, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, 26)\n",
        "\n",
        "        # Xavier initialization\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        nn.init.xavier_uniform_(self.fc3.weight)\n",
        "\n",
        "        self.fc1.bias.data.zero_()\n",
        "        self.fc2.bias.data.zero_()\n",
        "        self.fc3.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "\n",
        "class RobustAgent:\n",
        "    \"\"\"Debugged, stable agent\"\"\"\n",
        "\n",
        "    def __init__(self, hmm):\n",
        "        self.hmm = hmm\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 1.0  # Start high\n",
        "        self.eps_decay = 0.9995\n",
        "        self.eps_min = 0.05\n",
        "\n",
        "        # Networks\n",
        "        self.policy = StableDQN().to(device)\n",
        "        self.target = StableDQN().to(device)\n",
        "        self.target.load_state_dict(self.policy.state_dict())\n",
        "        self.target.eval()\n",
        "\n",
        "        # Optimizer with smaller learning rate\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=0.0001)\n",
        "\n",
        "        # Memory\n",
        "        self.memory = deque(maxlen=50000)\n",
        "        self.batch_size = 64\n",
        "\n",
        "        # Stats\n",
        "        self.steps = 0\n",
        "        self.stats = {'wins': 0, 'games': 0, 'wrong': 0, 'rep': 0}\n",
        "\n",
        "        # English frequency for bootstrapping\n",
        "        self.freq = np.array([\n",
        "            0.082, 0.015, 0.028, 0.043, 0.127, 0.022, 0.020, 0.061, 0.070,\n",
        "            0.002, 0.008, 0.040, 0.024, 0.067, 0.075, 0.019, 0.001, 0.060,\n",
        "            0.063, 0.091, 0.028, 0.010, 0.024, 0.002, 0.020, 0.001\n",
        "        ])\n",
        "\n",
        "    def get_state(self, pattern, guessed, wrong_left):\n",
        "        \"\"\"Simple 52-dim state\"\"\"\n",
        "        # HMM probs (26)\n",
        "        hmm = self.hmm.get_probs(pattern, guessed)\n",
        "\n",
        "        # Guessed mask (26)\n",
        "        mask = np.zeros(26)\n",
        "        for l in guessed:\n",
        "            mask[SYM2IDX[l]] = 1\n",
        "\n",
        "        state = np.concatenate([hmm, mask])\n",
        "        return state.astype(np.float32)\n",
        "\n",
        "    def select_action(self, state, guessed, training=True):\n",
        "        \"\"\"Select action with fallback to frequency\"\"\"\n",
        "        available = [i for i in range(26) if ALPHABET[i] not in guessed]\n",
        "        if not available:\n",
        "            return None\n",
        "\n",
        "        # Epsilon greedy\n",
        "        if training and random.random() < self.epsilon:\n",
        "            # Smart exploration: use HMM + frequency\n",
        "            hmm = state[:26]\n",
        "            combined = 0.6 * hmm + 0.4 * self.freq\n",
        "            combined = np.array([combined[i] if i in available else 0 for i in range(26)])\n",
        "            if combined.sum() > 0:\n",
        "                combined /= combined.sum()\n",
        "                return np.random.choice(26, p=combined)\n",
        "            return random.choice(available)\n",
        "\n",
        "        # Exploit: use network\n",
        "        with torch.no_grad():\n",
        "            state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "            q = self.policy(state_t).cpu().numpy()[0]\n",
        "\n",
        "            # Mask unavailable\n",
        "            for i in range(26):\n",
        "                if i not in available:\n",
        "                    q[i] = -1e9\n",
        "\n",
        "            return np.argmax(q)\n",
        "\n",
        "    def remember(self, s, a, r, ns, done):\n",
        "        \"\"\"Store transition\"\"\"\n",
        "        self.memory.append((s, a, r, ns, done))\n",
        "\n",
        "    def train_step(self):\n",
        "        \"\"\"Single training step\"\"\"\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return 0\n",
        "\n",
        "        # Sample\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "\n",
        "        # Unpack and convert to numpy first (fixes warning)\n",
        "        states_list, actions_list, rewards_list, next_states_list, dones_list = zip(*batch)\n",
        "\n",
        "        states = torch.FloatTensor(np.array(states_list, dtype=np.float32)).to(device)\n",
        "        actions = torch.LongTensor(actions_list).to(device)\n",
        "        rewards = torch.FloatTensor(rewards_list).to(device)\n",
        "        next_states = torch.FloatTensor(np.array(next_states_list, dtype=np.float32)).to(device)\n",
        "        dones = torch.FloatTensor(dones_list).to(device)\n",
        "\n",
        "        # Current Q\n",
        "        q_vals = self.policy(states)\n",
        "        q_vals = q_vals.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        # Target Q\n",
        "        with torch.no_grad():\n",
        "            next_q = self.target(next_states).max(1)[0]\n",
        "            target = rewards + (1 - dones) * self.gamma * next_q\n",
        "\n",
        "        # Clip targets to prevent explosion\n",
        "        target = torch.clamp(target, -100, 100)\n",
        "\n",
        "        # Loss\n",
        "        loss = F.mse_loss(q_vals, target)\n",
        "\n",
        "        # Optimize\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def train_episode(self, word):\n",
        "        \"\"\"Play one game\"\"\"\n",
        "        pattern = list('_' * len(word))\n",
        "        guessed = set()\n",
        "        wrong = 0\n",
        "        repeated = 0\n",
        "\n",
        "        while wrong < 6:\n",
        "            # Win check\n",
        "            if '_' not in pattern:\n",
        "                self.stats['wins'] += 1\n",
        "                return True, wrong, repeated\n",
        "\n",
        "            # State\n",
        "            state = self.get_state(''.join(pattern), guessed, 6 - wrong)\n",
        "\n",
        "            # Action\n",
        "            action = self.select_action(state, guessed, training=True)\n",
        "            if action is None:\n",
        "                break\n",
        "\n",
        "            letter = ALPHABET[action]\n",
        "\n",
        "            # Repeated?\n",
        "            if letter in guessed:\n",
        "                repeated += 1\n",
        "                reward = -10\n",
        "                self.remember(state, action, reward, state, False)\n",
        "                continue\n",
        "\n",
        "            guessed.add(letter)\n",
        "\n",
        "            # Correct?\n",
        "            if letter in word:\n",
        "                count = word.count(letter)\n",
        "                for i, c in enumerate(word):\n",
        "                    if c == letter:\n",
        "                        pattern[i] = letter\n",
        "                reward = 10 * count\n",
        "            else:\n",
        "                wrong += 1\n",
        "                reward = -5\n",
        "\n",
        "            # Next state\n",
        "            next_state = self.get_state(''.join(pattern), guessed, 6 - wrong)\n",
        "            done = (wrong >= 6)\n",
        "\n",
        "            self.remember(state, action, reward, next_state, done)\n",
        "\n",
        "            # Train every 4 steps\n",
        "            if self.steps % 4 == 0:\n",
        "                self.train_step()\n",
        "\n",
        "            self.steps += 1\n",
        "\n",
        "            # Update target every 500 steps\n",
        "            if self.steps % 500 == 0:\n",
        "                self.target.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        return False, wrong, repeated\n",
        "\n",
        "    def train(self, words, episodes=10000):\n",
        "        \"\"\"Train agent\"\"\"\n",
        "        print(f\"\\nüéì Training for {episodes} episodes\")\n",
        "\n",
        "        # Split train/val\n",
        "        random.shuffle(words)\n",
        "        split = int(0.9 * len(words))\n",
        "        train_words = words[:split]\n",
        "        val_words = words[split:]\n",
        "\n",
        "        best_score = -1e9\n",
        "\n",
        "        for ep in range(episodes):\n",
        "            # Sample word\n",
        "            word = random.choice(train_words)\n",
        "\n",
        "            # Train\n",
        "            won, wrong, rep = self.train_episode(word)\n",
        "\n",
        "            # Stats\n",
        "            self.stats['games'] += 1\n",
        "            self.stats['wrong'] += wrong\n",
        "            self.stats['rep'] += rep\n",
        "\n",
        "            # Decay epsilon\n",
        "            self.epsilon = max(self.eps_min, self.epsilon * self.eps_decay)\n",
        "\n",
        "            # Report\n",
        "            if (ep + 1) % 500 == 0:\n",
        "                g = self.stats['games']\n",
        "                w = self.stats['wins']\n",
        "                wr = w / g * 100\n",
        "                avg_wrong = self.stats['wrong'] / g\n",
        "                avg_rep = self.stats['rep'] / g\n",
        "\n",
        "                print(f\"\\nüìä Episode {ep+1}/{episodes}\")\n",
        "                print(f\"   Win rate: {wr:.1f}%\")\n",
        "                print(f\"   Avg wrong: {avg_wrong:.2f}\")\n",
        "                print(f\"   Avg rep: {avg_rep:.3f}\")\n",
        "                print(f\"   Epsilon: {self.epsilon:.3f}\")\n",
        "                print(f\"   Memory: {len(self.memory)}\")\n",
        "\n",
        "                # Validate\n",
        "                if len(val_words) > 0:\n",
        "                    score = self.validate(val_words[:100])\n",
        "                    print(f\"   Val score: {score:.0f}\")\n",
        "\n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "                        self.save(\"best_agent.pkl\")\n",
        "                        print(f\"   ‚úÖ Saved!\")\n",
        "\n",
        "        print(f\"\\n‚úÖ Done! Final WR: {self.stats['wins']/self.stats['games']*100:.1f}%\")\n",
        "\n",
        "    def play(self, word):\n",
        "        \"\"\"Play without training\"\"\"\n",
        "        pattern = list('_' * len(word))\n",
        "        guessed = set()\n",
        "        wrong = 0\n",
        "        rep = 0\n",
        "\n",
        "        while wrong < 6:\n",
        "            if '_' not in pattern:\n",
        "                return True, wrong, rep\n",
        "\n",
        "            state = self.get_state(''.join(pattern), guessed, 6 - wrong)\n",
        "            action = self.select_action(state, guessed, training=False)\n",
        "\n",
        "            if action is None:\n",
        "                break\n",
        "\n",
        "            letter = ALPHABET[action]\n",
        "\n",
        "            if letter in guessed:\n",
        "                rep += 1\n",
        "                continue\n",
        "\n",
        "            guessed.add(letter)\n",
        "\n",
        "            if letter in word:\n",
        "                for i, c in enumerate(word):\n",
        "                    if c == letter:\n",
        "                        pattern[i] = letter\n",
        "            else:\n",
        "                wrong += 1\n",
        "\n",
        "        return False, wrong, rep\n",
        "\n",
        "    def validate(self, words):\n",
        "        \"\"\"Quick validation\"\"\"\n",
        "        old_eps = self.epsilon\n",
        "        self.epsilon = 0\n",
        "\n",
        "        wins = 0\n",
        "        total_wrong = 0\n",
        "        total_rep = 0\n",
        "\n",
        "        for word in words:\n",
        "            won, wrong, rep = self.play(word)\n",
        "            if won:\n",
        "                wins += 1\n",
        "            total_wrong += wrong\n",
        "            total_rep += rep\n",
        "\n",
        "        self.epsilon = old_eps\n",
        "\n",
        "        sr = wins / len(words)\n",
        "        score = (sr * 2000) - (total_wrong * 5) - (total_rep * 2)\n",
        "        return score\n",
        "\n",
        "    def evaluate(self, words):\n",
        "        \"\"\"Final eval\"\"\"\n",
        "        print(f\"\\nüìä Evaluating {len(words)} words...\")\n",
        "\n",
        "        self.epsilon = 0\n",
        "        wins = 0\n",
        "        total_wrong = 0\n",
        "        total_rep = 0\n",
        "\n",
        "        for i, word in enumerate(words):\n",
        "            won, wrong, rep = self.play(word)\n",
        "            if won:\n",
        "                wins += 1\n",
        "            total_wrong += wrong\n",
        "            total_rep += rep\n",
        "\n",
        "            if (i + 1) % 500 == 0:\n",
        "                print(f\"   {i+1}/{len(words)}\")\n",
        "\n",
        "        sr = wins / len(words)\n",
        "        score = (sr * 2000) - (total_wrong * 5) - (total_rep * 2)\n",
        "\n",
        "        print(f\"\\nüìà RESULTS:\")\n",
        "        print(f\"   Wins: {wins}/{len(words)} ({sr*100:.1f}%)\")\n",
        "        print(f\"   Wrong: {total_wrong} (avg {total_wrong/len(words):.2f})\")\n",
        "        print(f\"   Repeated: {total_rep} (avg {total_rep/len(words):.3f})\")\n",
        "        print(f\"   üèÜ SCORE: {score:.2f}\")\n",
        "\n",
        "        return score\n",
        "\n",
        "    def save(self, path):\n",
        "        torch.save({\n",
        "            'policy': self.policy.state_dict(),\n",
        "            'target': self.target.state_dict(),\n",
        "            'eps': self.epsilon,\n",
        "            'stats': self.stats\n",
        "        }, path)\n",
        "\n",
        "    def load(self, path):\n",
        "        ckpt = torch.load(path, map_location=device)\n",
        "        self.policy.load_state_dict(ckpt['policy'])\n",
        "        self.target.load_state_dict(ckpt['target'])\n",
        "        self.epsilon = ckpt['eps']\n",
        "        self.stats = ckpt['stats']\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"=\"*60)\n",
        "    print(\"üéØ STABLE HMM + DQN AGENT\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load HMM\n",
        "    print(\"\\n1Ô∏è‚É£ Loading HMM...\")\n",
        "    with open(\"hmm_trained.pkl\", \"rb\") as f:\n",
        "        model = pickle.load(f)\n",
        "\n",
        "    hmm = SimpleHMM(model['pi'], model['A'], model['B'])\n",
        "\n",
        "    # Load words\n",
        "    print(\"\\n2Ô∏è‚É£ Loading words...\")\n",
        "    with open(\"corpus.txt\") as f:\n",
        "        words = [line.strip().upper() for line in f]\n",
        "    words = [''.join(c for c in w if c in SYM2IDX) for w in words if len(w) > 0]\n",
        "    print(f\"   {len(words)} words\")\n",
        "\n",
        "    # Train\n",
        "    print(\"\\n3Ô∏è‚É£ Creating agent...\")\n",
        "    agent = RobustAgent(hmm)\n",
        "\n",
        "    agent.train(words, episodes=10000)\n",
        "\n",
        "    # Test\n",
        "    print(\"\\n4Ô∏è‚É£ Testing...\")\n",
        "    with open(\"test.txt\") as f:\n",
        "        test = [line.strip().upper() for line in f]\n",
        "    test = [''.join(c for c in w if c in SYM2IDX) for w in test if len(w) > 0]\n",
        "\n",
        "    agent.evaluate(test[:2000])\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm7TWBO073kX",
        "outputId": "2ebfeed1-3288-4894-9b98-dd154bfd1f61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Device: cpu\n",
            "============================================================\n",
            "üéØ STABLE HMM + DQN AGENT\n",
            "============================================================\n",
            "\n",
            "1Ô∏è‚É£ Loading HMM...\n",
            "\n",
            "2Ô∏è‚É£ Loading words...\n",
            "   50000 words\n",
            "\n",
            "3Ô∏è‚É£ Creating agent...\n",
            "\n",
            "üéì Training for 10000 episodes\n",
            "\n",
            "üìä Episode 500/10000\n",
            "   Win rate: 10.2%\n",
            "   Avg wrong: 5.82\n",
            "   Avg rep: 0.000\n",
            "   Epsilon: 0.779\n",
            "   Memory: 5265\n",
            "   Val score: -2465\n",
            "   ‚úÖ Saved!\n",
            "\n",
            "üìä Episode 1000/10000\n",
            "   Win rate: 11.8%\n",
            "   Avg wrong: 5.79\n",
            "   Avg rep: 0.000\n",
            "   Epsilon: 0.606\n",
            "   Memory: 10645\n",
            "   Val score: -2460\n",
            "   ‚úÖ Saved!\n",
            "\n",
            "üìä Episode 1500/10000\n",
            "   Win rate: 11.1%\n",
            "   Avg wrong: 5.79\n",
            "   Avg rep: 0.000\n",
            "   Epsilon: 0.472\n",
            "   Memory: 16038\n",
            "   Val score: -2415\n",
            "   ‚úÖ Saved!\n",
            "\n",
            "üìä Episode 2000/10000\n",
            "   Win rate: 11.4%\n",
            "   Avg wrong: 5.78\n",
            "   Avg rep: 0.000\n",
            "   Epsilon: 0.368\n",
            "   Memory: 21547\n",
            "   Val score: -2745\n",
            "\n",
            "üìä Episode 2500/10000\n",
            "   Win rate: 11.3%\n",
            "   Avg wrong: 5.78\n",
            "   Avg rep: 0.000\n",
            "   Epsilon: 0.286\n",
            "   Memory: 27153\n",
            "   Val score: -2755\n",
            "\n",
            "üìä Episode 3000/10000\n",
            "   Win rate: 10.9%\n",
            "   Avg wrong: 5.79\n",
            "   Avg rep: 0.000\n",
            "   Epsilon: 0.223\n",
            "   Memory: 32623\n",
            "   Val score: -2900\n",
            "\n",
            "üìä Episode 3500/10000\n",
            "   Win rate: 10.3%\n",
            "   Avg wrong: 5.80\n",
            "   Avg rep: 0.000\n",
            "   Epsilon: 0.174\n",
            "   Memory: 37906\n",
            "   Val score: -2905\n",
            "\n",
            "üìä Episode 4000/10000\n",
            "   Win rate: 10.2%\n",
            "   Avg wrong: 5.80\n",
            "   Avg rep: 0.000\n",
            "   Epsilon: 0.135\n",
            "   Memory: 43371\n",
            "   Val score: -2555\n",
            "\n",
            "üìä Episode 4500/10000\n",
            "   Win rate: 10.5%\n",
            "   Avg wrong: 5.79\n",
            "   Avg rep: 0.000\n",
            "   Epsilon: 0.105\n",
            "   Memory: 48917\n",
            "   Val score: -2475\n",
            "\n",
            "üìä Episode 5000/10000\n",
            "   Win rate: 10.9%\n",
            "   Avg wrong: 5.79\n",
            "   Avg rep: 0.000\n",
            "   Epsilon: 0.082\n",
            "   Memory: 50000\n",
            "   Val score: -2355\n",
            "   ‚úÖ Saved!\n",
            "\n",
            "üìä Episode 5500/10000\n",
            "   Win rate: 11.5%\n",
            "   Avg wrong: 5.77\n",
            "   Avg rep: 0.000\n",
            "   Epsilon: 0.064\n",
            "   Memory: 50000\n",
            "   Val score: -2425\n",
            "\n",
            "üìä Episode 6000/10000\n",
            "   Win rate: 11.6%\n",
            "   Avg wrong: 5.77\n",
            "   Avg rep: 0.000\n",
            "   Epsilon: 0.050\n",
            "   Memory: 50000\n",
            "   Val score: -2385\n",
            "\n",
            "üìä Episode 6500/10000\n",
            "   Win rate: 11.8%\n",
            "   Avg wrong: 5.77\n",
            "   Avg rep: 0.000\n",
            "   Epsilon: 0.050\n",
            "   Memory: 50000\n",
            "   Val score: -2340\n",
            "   ‚úÖ Saved!\n",
            "\n",
            "üìä Episode 7000/10000\n",
            "   Win rate: 12.0%\n",
            "   Avg wrong: 5.77\n",
            "   Avg rep: 0.000\n",
            "   Epsilon: 0.050\n",
            "   Memory: 50000\n",
            "   Val score: -2360\n",
            "\n",
            "üìä Episode 7500/10000\n",
            "   Win rate: 12.2%\n",
            "   Avg wrong: 5.76\n",
            "   Avg rep: 0.000\n",
            "   Epsilon: 0.050\n",
            "   Memory: 50000\n",
            "   Val score: -2365\n",
            "\n",
            "üìä Episode 8000/10000\n",
            "   Win rate: 12.3%\n",
            "   Avg wrong: 5.76\n",
            "   Avg rep: 0.000\n",
            "   Epsilon: 0.050\n",
            "   Memory: 50000\n",
            "   Val score: -2350\n",
            "\n",
            "üìä Episode 8500/10000\n",
            "   Win rate: 12.4%\n",
            "   Avg wrong: 5.75\n",
            "   Avg rep: 0.000\n",
            "   Epsilon: 0.050\n",
            "   Memory: 50000\n",
            "   Val score: -2400\n",
            "\n",
            "üìä Episode 9000/10000\n",
            "   Win rate: 12.5%\n",
            "   Avg wrong: 5.75\n",
            "   Avg rep: 0.000\n",
            "   Epsilon: 0.050\n",
            "   Memory: 50000\n",
            "   Val score: -2390\n",
            "\n",
            "üìä Episode 9500/10000\n",
            "   Win rate: 12.6%\n",
            "   Avg wrong: 5.75\n",
            "   Avg rep: 0.000\n",
            "   Epsilon: 0.050\n",
            "   Memory: 50000\n",
            "   Val score: -2425\n",
            "\n",
            "üìä Episode 10000/10000\n",
            "   Win rate: 12.7%\n",
            "   Avg wrong: 5.75\n",
            "   Avg rep: 0.000\n",
            "   Epsilon: 0.050\n",
            "   Memory: 50000\n",
            "   Val score: -2410\n",
            "\n",
            "‚úÖ Done! Final WR: 12.7%\n",
            "\n",
            "4Ô∏è‚É£ Testing...\n",
            "\n",
            "üìä Evaluating 2000 words...\n",
            "   500/2000\n",
            "   1000/2000\n",
            "   1500/2000\n",
            "   2000/2000\n",
            "\n",
            "üìà RESULTS:\n",
            "   Wins: 309/2000 (15.4%)\n",
            "   Wrong: 11380 (avg 5.69)\n",
            "   Repeated: 0 (avg 0.000)\n",
            "   üèÜ SCORE: -56591.00\n",
            "\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import string\n",
        "import pickle\n",
        "from collections import Counter, defaultdict\n",
        "import random\n",
        "\n",
        "ALPHABET = list(string.ascii_uppercase)\n",
        "SYM2IDX = {c: i for i, c in enumerate(ALPHABET)}\n",
        "IDX2SYM = {i: c for c, i in SYM2IDX.items()}\n",
        "\n",
        "\n",
        "class HMMFeatureExtractor:\n",
        "    \"\"\"\n",
        "    Extracts HMM-based features for RL state representation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pi, A, B):\n",
        "        self.pi = pi\n",
        "        self.A = A\n",
        "        self.B = B\n",
        "        self.N = len(pi)\n",
        "\n",
        "    def get_letter_probabilities(self, pattern, guessed_letters):\n",
        "        \"\"\"\n",
        "        Get HMM probability distribution over all 26 letters\n",
        "        Returns: vector of 26 probabilities\n",
        "        \"\"\"\n",
        "        length = len(pattern)\n",
        "\n",
        "        # Convert pattern to observation indices (-1 for unknown)\n",
        "        obs = []\n",
        "        unknown_positions = []\n",
        "        for i, c in enumerate(pattern):\n",
        "            if c == '_':\n",
        "                obs.append(-1)\n",
        "                unknown_positions.append(i)\n",
        "            else:\n",
        "                obs.append(SYM2IDX[c])\n",
        "\n",
        "        if not unknown_positions:\n",
        "            return np.zeros(26)\n",
        "\n",
        "        # Run forward-backward\n",
        "        alpha = self._forward(obs)\n",
        "        beta = self._backward(obs)\n",
        "\n",
        "        # Compute marginal probabilities for each letter\n",
        "        letter_probs = np.zeros(26)\n",
        "\n",
        "        for pos in unknown_positions:\n",
        "            gamma = alpha[pos] * beta[pos]\n",
        "            gamma = gamma / (np.sum(gamma) + 1e-300)\n",
        "\n",
        "            # Accumulate emission probabilities\n",
        "            for letter_idx in range(26):\n",
        "                letter_probs[letter_idx] += np.sum(gamma * self.B[:, letter_idx])\n",
        "\n",
        "        # Normalize\n",
        "        total = np.sum(letter_probs)\n",
        "        if total > 0:\n",
        "            letter_probs = letter_probs / total\n",
        "\n",
        "        # Zero out already guessed letters\n",
        "        for letter in guessed_letters:\n",
        "            letter_probs[SYM2IDX[letter]] = 0\n",
        "\n",
        "        return letter_probs\n",
        "\n",
        "    def get_state_distribution(self, pattern):\n",
        "        \"\"\"\n",
        "        Get HMM hidden state distribution given pattern\n",
        "        Returns: vector of state probabilities\n",
        "        \"\"\"\n",
        "        obs = []\n",
        "        for c in pattern:\n",
        "            if c == '_':\n",
        "                obs.append(-1)\n",
        "            else:\n",
        "                obs.append(SYM2IDX[c])\n",
        "\n",
        "        alpha = self._forward(obs)\n",
        "        return alpha[-1] / (np.sum(alpha[-1]) + 1e-300)\n",
        "\n",
        "    def _forward(self, obs):\n",
        "        \"\"\"Forward algorithm with unknown observations\"\"\"\n",
        "        T = len(obs)\n",
        "        alpha = np.zeros((T, self.N))\n",
        "\n",
        "        # Initialize\n",
        "        if obs[0] == -1:\n",
        "            alpha[0] = self.pi\n",
        "        else:\n",
        "            alpha[0] = self.pi * self.B[:, obs[0]]\n",
        "        alpha[0] = alpha[0] / (np.sum(alpha[0]) + 1e-300)\n",
        "\n",
        "        # Forward pass\n",
        "        for t in range(1, T):\n",
        "            alpha[t] = alpha[t - 1] @ self.A\n",
        "            if obs[t] != -1:\n",
        "                alpha[t] = alpha[t] * self.B[:, obs[t]]\n",
        "            alpha[t] = alpha[t] / (np.sum(alpha[t]) + 1e-300)\n",
        "\n",
        "        return alpha\n",
        "\n",
        "    def _backward(self, obs):\n",
        "        \"\"\"Backward algorithm with unknown observations\"\"\"\n",
        "        T = len(obs)\n",
        "        beta = np.zeros((T, self.N))\n",
        "        beta[-1] = 1.0\n",
        "\n",
        "        for t in range(T - 2, -1, -1):\n",
        "            if obs[t + 1] != -1:\n",
        "                beta[t] = self.A @ (self.B[:, obs[t + 1]] * beta[t + 1])\n",
        "            else:\n",
        "                beta[t] = self.A @ beta[t + 1]\n",
        "            beta[t] = beta[t] / (np.sum(beta[t]) + 1e-300)\n",
        "\n",
        "        return beta\n",
        "\n",
        "\n",
        "class HangmanQLearningAgent:\n",
        "    \"\"\"\n",
        "    Q-Learning agent for Hangman that uses HMM features\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hmm_extractor, learning_rate=0.1, discount=0.95,\n",
        "                 epsilon=0.1, epsilon_decay=0.995, epsilon_min=0.01):\n",
        "        self.hmm = hmm_extractor\n",
        "        self.alpha = learning_rate\n",
        "        self.gamma = discount\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "\n",
        "        # Q-table: state -> action -> value\n",
        "        self.Q = defaultdict(lambda: np.zeros(26))\n",
        "\n",
        "        # For tracking performance\n",
        "        self.training_stats = {\n",
        "            'games_played': 0,\n",
        "            'games_won': 0,\n",
        "            'total_wrong_guesses': 0,\n",
        "            'total_repeated_guesses': 0\n",
        "        }\n",
        "\n",
        "    def get_state_key_simple(self, pattern, guessed_letters, wrong_guesses_left):\n",
        "        \"\"\"\n",
        "        Simpler state representation for smaller state space\n",
        "        \"\"\"\n",
        "        # Get HMM probabilities\n",
        "        hmm_probs = self.hmm.get_letter_probabilities(pattern, guessed_letters)\n",
        "\n",
        "        # Discretize HMM probs into bins\n",
        "        top_3_indices = np.argsort(hmm_probs)[-3:][::-1]\n",
        "        top_3_letters = tuple(ALPHABET[i] for i in top_3_indices)\n",
        "\n",
        "        # Pattern features\n",
        "        length = len(pattern)\n",
        "        known_count = sum(1 for c in pattern if c != '_')\n",
        "        known_ratio = int(known_count / length * 10)  # 0-10\n",
        "\n",
        "        state_key = (\n",
        "            length,\n",
        "            known_ratio,\n",
        "            wrong_guesses_left,\n",
        "            top_3_letters,\n",
        "            tuple(sorted(guessed_letters))\n",
        "        )\n",
        "\n",
        "        return state_key\n",
        "\n",
        "    def choose_action(self, state_key, guessed_letters, hmm_probs, training=True):\n",
        "        \"\"\"\n",
        "        Choose action using epsilon-greedy with HMM guidance\n",
        "        \"\"\"\n",
        "        available_letters = [i for i in range(26) if ALPHABET[i] not in guessed_letters]\n",
        "\n",
        "        if not available_letters:\n",
        "            return None\n",
        "\n",
        "        # Epsilon-greedy exploration\n",
        "        if training and random.random() < self.epsilon:\n",
        "            # Explore: weighted random by HMM probs\n",
        "            probs = np.array([hmm_probs[i] for i in available_letters])\n",
        "            if np.sum(probs) > 0:\n",
        "                probs = probs / np.sum(probs)\n",
        "                action = np.random.choice(available_letters, p=probs)\n",
        "            else:\n",
        "                action = random.choice(available_letters)\n",
        "        else:\n",
        "            # Exploit: choose best Q-value among available\n",
        "            q_values = self.Q[state_key]\n",
        "\n",
        "            # Combine Q-values with HMM probs for better decisions\n",
        "            combined_scores = np.zeros(26)\n",
        "            for i in available_letters:\n",
        "                combined_scores[i] = 0.7 * q_values[i] + 0.3 * hmm_probs[i]\n",
        "\n",
        "            action = np.argmax(combined_scores)\n",
        "\n",
        "            # If Q-value is 0 (unvisited), fall back to HMM\n",
        "            if q_values[action] == 0:\n",
        "                action = np.argmax(hmm_probs)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def update_q_value(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Q-learning update: Q(s,a) += Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]\n",
        "        \"\"\"\n",
        "        current_q = self.Q[state][action]\n",
        "\n",
        "        if done:\n",
        "            max_next_q = 0\n",
        "        else:\n",
        "            max_next_q = np.max(self.Q[next_state])\n",
        "\n",
        "        new_q = current_q + self.alpha * (reward + self.gamma * max_next_q - current_q)\n",
        "        self.Q[state][action] = new_q\n",
        "\n",
        "    def train_episode(self, word, max_wrong=6):\n",
        "        \"\"\"\n",
        "        Train on a single game of Hangman with reward shaping\n",
        "        \"\"\"\n",
        "        pattern = ['_'] * len(word)\n",
        "        guessed_letters = set()\n",
        "        wrong_guesses = 0\n",
        "        repeated_guesses = 0\n",
        "\n",
        "        episode_history = []  # (state, action, reward)\n",
        "\n",
        "        while wrong_guesses < max_wrong:\n",
        "            if '_' not in pattern:\n",
        "                break\n",
        "\n",
        "            # Get current state\n",
        "            pattern_str = ''.join(pattern)\n",
        "            hmm_probs = self.hmm.get_letter_probabilities(pattern_str, guessed_letters)\n",
        "            state = self.get_state_key_simple(pattern_str, guessed_letters, max_wrong - wrong_guesses)\n",
        "\n",
        "            # Choose action\n",
        "            action = self.choose_action(state, guessed_letters, hmm_probs, training=True)\n",
        "            if action is None:\n",
        "                break\n",
        "\n",
        "            letter = ALPHABET[action]\n",
        "            reward = 0\n",
        "\n",
        "            # --- Reward shaping logic ---\n",
        "            if letter in guessed_letters:\n",
        "                repeated_guesses += 1\n",
        "                reward = -8\n",
        "                episode_history.append((state, action, reward))\n",
        "                continue\n",
        "\n",
        "            guessed_letters.add(letter)\n",
        "\n",
        "            if letter in word:\n",
        "                new_letters = sum(1 for i, c in enumerate(word) if c == letter and pattern[i] == '_')\n",
        "                for i, c in enumerate(word):\n",
        "                    if c == letter:\n",
        "                        pattern[i] = letter\n",
        "                reward = 10 * new_letters\n",
        "            else:\n",
        "                wrong_guesses += 1\n",
        "                reward = -4\n",
        "\n",
        "            if '_' not in pattern:\n",
        "                reward += 50\n",
        "            elif wrong_guesses >= max_wrong:\n",
        "                reward -= 50\n",
        "\n",
        "            episode_history.append((state, action, reward))\n",
        "\n",
        "            if wrong_guesses >= max_wrong or '_' not in pattern:\n",
        "                break\n",
        "\n",
        "        self._backpropagate_rewards(episode_history, reward)\n",
        "\n",
        "        won = '_' not in pattern\n",
        "        if won:\n",
        "            self.training_stats['games_won'] += 1\n",
        "\n",
        "        return won, wrong_guesses, repeated_guesses\n",
        "\n",
        "    def _backpropagate_rewards(self, episode_history, final_reward):\n",
        "        \"\"\"\n",
        "        Backpropagate rewards through episode\n",
        "        \"\"\"\n",
        "        for i, (state, action, immediate_reward) in enumerate(episode_history):\n",
        "            remaining_steps = len(episode_history) - i - 1\n",
        "            discounted_final = final_reward * (self.gamma ** remaining_steps)\n",
        "            total_reward = immediate_reward + discounted_final\n",
        "\n",
        "            if i < len(episode_history) - 1:\n",
        "                next_state = episode_history[i + 1][0]\n",
        "                done = False\n",
        "            else:\n",
        "                next_state = state\n",
        "                done = True\n",
        "\n",
        "            self.update_q_value(state, action, total_reward, next_state, done)\n",
        "\n",
        "    def train(self, word_list, episodes=5000):\n",
        "        \"\"\"\n",
        "        Train the Q-learning agent on a list of words\n",
        "        \"\"\"\n",
        "        print(f\"\\nüéì Training Q-Learning Agent for {episodes} episodes...\")\n",
        "        print(f\"   Word list size: {len(word_list)}\")\n",
        "\n",
        "        for episode in range(episodes):\n",
        "            word = random.choice(word_list).upper()\n",
        "            word = ''.join([c for c in word if c in SYM2IDX])\n",
        "\n",
        "            if len(word) == 0:\n",
        "                continue\n",
        "\n",
        "            won, wrong, repeated = self.train_episode(word)\n",
        "\n",
        "            self.training_stats['games_played'] += 1\n",
        "            self.training_stats['total_wrong_guesses'] += wrong\n",
        "            self.training_stats['total_repeated_guesses'] += repeated\n",
        "\n",
        "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "            if (episode + 1) % 500 == 0:\n",
        "                games = self.training_stats['games_played']\n",
        "                wins = self.training_stats['games_won']\n",
        "                win_rate = wins / games * 100 if games > 0 else 0\n",
        "                avg_wrong = self.training_stats['total_wrong_guesses'] / games\n",
        "                avg_repeated = self.training_stats['total_repeated_guesses'] / games\n",
        "\n",
        "                print(f\"\\n   Episode {episode + 1}/{episodes}\")\n",
        "                print(f\"   Win rate: {win_rate:.2f}%\")\n",
        "                print(f\"   Avg wrong guesses: {avg_wrong:.2f}\")\n",
        "                print(f\"   Avg repeated guesses: {avg_repeated:.2f}\")\n",
        "                print(f\"   Epsilon: {self.epsilon:.4f}\")\n",
        "                print(f\"   Q-table size: {len(self.Q)}\")\n",
        "\n",
        "        print(f\"\\n‚úÖ Training complete!\")\n",
        "        print(f\"   Final win rate: {self.training_stats['games_won'] / self.training_stats['games_played'] * 100:.2f}%\")\n",
        "        print(f\"   Q-table states: {len(self.Q)}\")\n",
        "\n",
        "    def play_game(self, word, max_wrong=6, verbose=False):\n",
        "        \"\"\"\n",
        "        Play a single game (testing/evaluation)\n",
        "        \"\"\"\n",
        "        pattern = ['_'] * len(word)\n",
        "        guessed_letters = set()\n",
        "        wrong_guesses = 0\n",
        "        repeated_guesses = 0\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nüéÆ Playing: {word}\")\n",
        "\n",
        "        while wrong_guesses < max_wrong:\n",
        "            if '_' not in pattern:\n",
        "                if verbose:\n",
        "                    print(f\"   ‚úÖ WON! Wrong: {wrong_guesses}, Repeated: {repeated_guesses}\")\n",
        "                return True, wrong_guesses, repeated_guesses\n",
        "\n",
        "            pattern_str = ''.join(pattern)\n",
        "            hmm_probs = self.hmm.get_letter_probabilities(pattern_str, guessed_letters)\n",
        "            state = self.get_state_key_simple(pattern_str, guessed_letters, max_wrong - wrong_guesses)\n",
        "\n",
        "            action = self.choose_action(state, guessed_letters, hmm_probs, training=False)\n",
        "            if action is None:\n",
        "                break\n",
        "\n",
        "            letter = ALPHABET[action]\n",
        "\n",
        "            if letter in guessed_letters:\n",
        "                repeated_guesses += 1\n",
        "                if verbose:\n",
        "                    print(f\"   ‚ùå Repeated: {letter}\")\n",
        "                continue\n",
        "\n",
        "            guessed_letters.add(letter)\n",
        "\n",
        "            if letter in word:\n",
        "                for i, c in enumerate(word):\n",
        "                    if c == letter:\n",
        "                        pattern[i] = letter\n",
        "                if verbose:\n",
        "                    print(f\"   ‚úì {letter}: {''.join(pattern)}\")\n",
        "            else:\n",
        "                wrong_guesses += 1\n",
        "                if verbose:\n",
        "                    print(f\"   ‚úó {letter}: {''.join(pattern)} ({wrong_guesses}/{max_wrong})\")\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"   ‚ùå LOST! Wrong: {wrong_guesses}, Repeated: {repeated_guesses}\")\n",
        "        return False, wrong_guesses, repeated_guesses\n",
        "\n",
        "    def evaluate(self, test_words, max_wrong=6):\n",
        "        \"\"\"\n",
        "        Evaluate agent on test set\n",
        "        \"\"\"\n",
        "        print(f\"\\nüìä Evaluating on {len(test_words)} words...\")\n",
        "\n",
        "        wins = 0\n",
        "        total_wrong = 0\n",
        "        total_repeated = 0\n",
        "\n",
        "        for word in test_words:\n",
        "            won, wrong, repeated = self.play_game(word, max_wrong)\n",
        "            if won:\n",
        "                wins += 1\n",
        "            total_wrong += wrong\n",
        "            total_repeated += repeated\n",
        "\n",
        "        success_rate = wins / len(test_words)\n",
        "        final_score = (success_rate * 2000) - (total_wrong * 5) - (total_repeated * 2)\n",
        "\n",
        "        print(f\"\\nüìà RESULTS:\")\n",
        "        print(f\"   Games played: {len(test_words)}\")\n",
        "        print(f\"   Wins: {wins}\")\n",
        "        print(f\"   Success rate: {success_rate * 100:.2f}%\")\n",
        "        print(f\"   Total wrong guesses: {total_wrong}\")\n",
        "        print(f\"   Total repeated guesses: {total_repeated}\")\n",
        "        print(f\"   Avg wrong per game: {total_wrong / len(test_words):.2f}\")\n",
        "        print(f\"   Avg repeated per game: {total_repeated / len(test_words):.2f}\")\n",
        "        print(f\"\\n   üèÜ FINAL SCORE: {final_score:.2f}\")\n",
        "\n",
        "        return success_rate, total_wrong, total_repeated, final_score\n",
        "\n",
        "    def save(self, path=\"q_learning_agent.pkl\"):\n",
        "        \"\"\"Save Q-table and parameters\"\"\"\n",
        "        data = {\n",
        "            'Q': dict(self.Q),\n",
        "            'alpha': self.alpha,\n",
        "            'gamma': self.gamma,\n",
        "            'epsilon': self.epsilon,\n",
        "            'stats': self.training_stats\n",
        "        }\n",
        "        with open(path, 'wb') as f:\n",
        "            pickle.dump(data, f)\n",
        "        print(f\"‚úÖ Saved agent to {path}\")\n",
        "\n",
        "    def load(self, path=\"q_learning_agent.pkl\"):\n",
        "        \"\"\"Load Q-table and parameters\"\"\"\n",
        "        with open(path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        self.Q = defaultdict(lambda: np.zeros(26), data['Q'])\n",
        "        self.alpha = data['alpha']\n",
        "        self.gamma = data['gamma']\n",
        "        self.epsilon = data['epsilon']\n",
        "        self.training_stats = data['stats']\n",
        "        print(f\"‚úÖ Loaded agent from {path}\")\n",
        "\n",
        "\n",
        "# ========== MAIN USAGE ==========\n",
        "def main():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ü§ñ HMM + Q-Learning Hangman Agent\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Load HMM model\n",
        "    print(\"\\n1Ô∏è‚É£ Loading HMM model...\")\n",
        "    with open(\"hmm_trained.pkl\", \"rb\") as f:\n",
        "        model = pickle.load(f)\n",
        "    pi, A, B = model['pi'], model['A'], model['B']\n",
        "\n",
        "    # Create HMM feature extractor\n",
        "    hmm_extractor = HMMFeatureExtractor(pi, A, B)\n",
        "\n",
        "    # Load training words\n",
        "    print(\"\\n2Ô∏è‚É£ Loading training words...\")\n",
        "    with open(\"corpus.txt\", 'r', encoding='utf-8') as f:\n",
        "        train_words = [line.strip().upper() for line in f]\n",
        "    train_words = [''.join([c for c in w if c in SYM2IDX]) for w in train_words]\n",
        "    train_words = [w for w in train_words if len(w) > 0]\n",
        "    print(f\"   Loaded {len(train_words)} training words\")\n",
        "\n",
        "    # Create Q-learning agent\n",
        "    agent = HangmanQLearningAgent(\n",
        "        hmm_extractor,\n",
        "        learning_rate=0.1,\n",
        "        discount=0.95,\n",
        "        epsilon=0.3,\n",
        "        epsilon_decay=0.995,\n",
        "        epsilon_min=0.01\n",
        "    )\n",
        "\n",
        "    # Train agent\n",
        "    print(\"\\n3Ô∏è‚É£ Training agent...\")\n",
        "    agent.train(train_words, episodes=10000)\n",
        "\n",
        "    # Save agent\n",
        "    agent.save(\"hangman_qlearning_agent_v2.pkl\")\n",
        "\n",
        "    # Load test words\n",
        "    print(\"\\n4Ô∏è‚É£ Loading test words...\")\n",
        "    with open(\"test.txt\", 'r', encoding='utf-8') as f:\n",
        "        test_words = [line.strip().upper() for line in f]\n",
        "    test_words = [''.join([c for c in w if c in SYM2IDX]) for w in test_words]\n",
        "    test_words = [w for w in test_words if len(w) > 0]\n",
        "    print(f\"   Loaded {len(test_words)} test words\")\n",
        "\n",
        "    # Evaluate\n",
        "    print(\"\\n5Ô∏è‚É£ Evaluating agent...\")\n",
        "    agent.evaluate(test_words[:2000], max_wrong=6)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQC2JzEKLZoQ",
        "outputId": "4d1df604-e783-40cc-e7ac-a7f66a37fade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ü§ñ HMM + Q-Learning Hangman Agent\n",
            "============================================================\n",
            "\n",
            "1Ô∏è‚É£ Loading HMM model...\n",
            "\n",
            "2Ô∏è‚É£ Loading training words...\n",
            "   Loaded 50000 training words\n",
            "\n",
            "3Ô∏è‚É£ Training agent...\n",
            "\n",
            "üéì Training Q-Learning Agent for 10000 episodes...\n",
            "   Word list size: 50000\n",
            "\n",
            "   Episode 500/10000\n",
            "   Win rate: 33.60%\n",
            "   Avg wrong guesses: 5.23\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0245\n",
            "   Q-table size: 4495\n",
            "\n",
            "   Episode 1000/10000\n",
            "   Win rate: 31.70%\n",
            "   Avg wrong guesses: 5.25\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 8497\n",
            "\n",
            "   Episode 1500/10000\n",
            "   Win rate: 32.13%\n",
            "   Avg wrong guesses: 5.24\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 12384\n",
            "\n",
            "   Episode 2000/10000\n",
            "   Win rate: 31.80%\n",
            "   Avg wrong guesses: 5.25\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 16009\n",
            "\n",
            "   Episode 2500/10000\n",
            "   Win rate: 31.96%\n",
            "   Avg wrong guesses: 5.24\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 19404\n",
            "\n",
            "   Episode 3000/10000\n",
            "   Win rate: 32.63%\n",
            "   Avg wrong guesses: 5.23\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 22818\n",
            "\n",
            "   Episode 3500/10000\n",
            "   Win rate: 32.54%\n",
            "   Avg wrong guesses: 5.23\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 26184\n",
            "\n",
            "   Episode 4000/10000\n",
            "   Win rate: 32.52%\n",
            "   Avg wrong guesses: 5.23\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 29307\n",
            "\n",
            "   Episode 4500/10000\n",
            "   Win rate: 32.42%\n",
            "   Avg wrong guesses: 5.24\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 32436\n",
            "\n",
            "   Episode 5000/10000\n",
            "   Win rate: 32.90%\n",
            "   Avg wrong guesses: 5.23\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 35534\n",
            "\n",
            "   Episode 5500/10000\n",
            "   Win rate: 32.98%\n",
            "   Avg wrong guesses: 5.22\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 38503\n",
            "\n",
            "   Episode 6000/10000\n",
            "   Win rate: 33.10%\n",
            "   Avg wrong guesses: 5.22\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 41337\n",
            "\n",
            "   Episode 6500/10000\n",
            "   Win rate: 33.31%\n",
            "   Avg wrong guesses: 5.21\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 44211\n",
            "\n",
            "   Episode 7000/10000\n",
            "   Win rate: 33.50%\n",
            "   Avg wrong guesses: 5.21\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 47035\n",
            "\n",
            "   Episode 7500/10000\n",
            "   Win rate: 33.44%\n",
            "   Avg wrong guesses: 5.21\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 49732\n",
            "\n",
            "   Episode 8000/10000\n",
            "   Win rate: 33.46%\n",
            "   Avg wrong guesses: 5.20\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 52405\n",
            "\n",
            "   Episode 8500/10000\n",
            "   Win rate: 33.41%\n",
            "   Avg wrong guesses: 5.20\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 55172\n",
            "\n",
            "   Episode 9000/10000\n",
            "   Win rate: 33.31%\n",
            "   Avg wrong guesses: 5.20\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 57732\n",
            "\n",
            "   Episode 9500/10000\n",
            "   Win rate: 33.47%\n",
            "   Avg wrong guesses: 5.20\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 60374\n",
            "\n",
            "   Episode 10000/10000\n",
            "   Win rate: 33.58%\n",
            "   Avg wrong guesses: 5.19\n",
            "   Avg repeated guesses: 0.00\n",
            "   Epsilon: 0.0100\n",
            "   Q-table size: 63073\n",
            "\n",
            "‚úÖ Training complete!\n",
            "   Final win rate: 33.58%\n",
            "   Q-table states: 63073\n",
            "‚úÖ Saved agent to hangman_qlearning_agent_v2.pkl\n",
            "\n",
            "4Ô∏è‚É£ Loading test words...\n",
            "   Loaded 2000 test words\n",
            "\n",
            "5Ô∏è‚É£ Evaluating agent...\n",
            "\n",
            "üìä Evaluating on 2000 words...\n",
            "\n",
            "üìà RESULTS:\n",
            "   Games played: 2000\n",
            "   Wins: 695\n",
            "   Success rate: 34.75%\n",
            "   Total wrong guesses: 10286\n",
            "   Total repeated guesses: 0\n",
            "   Avg wrong per game: 5.14\n",
            "   Avg repeated per game: 0.00\n",
            "\n",
            "   üèÜ FINAL SCORE: -50735.00\n",
            "\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}